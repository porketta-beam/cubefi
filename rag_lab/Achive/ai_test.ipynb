{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08988bc",
   "metadata": {},
   "source": [
    "## retriever 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path='../.env', override=True)\n",
    "\n",
    "apikey = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb084f1",
   "metadata": {},
   "source": [
    "#### 문서 로더\n",
    "\n",
    "파일\n",
    "* TextLoader\n",
    "| 일반 텍스트 파일(.txt) 읽기\n",
    "* PDFPlumberLoader / PyPDFLoader\n",
    "| PDF 문서 불러오기\n",
    "* UnstructuredFileLoader\n",
    "| 다양한 파일 포맷 (Word, PPT, PDF 등)을 구조 없이 불러오기\n",
    "* CSVLoader\n",
    "| CSV 파일의 각 행을 문서로 변환\n",
    "* JSONLoader\n",
    "| JSON 파일을 읽고 문서로 구성\n",
    "\n",
    "\n",
    "웹/URL\n",
    "* WebBaseLoader\n",
    "| 일반 웹 페이지 크롤링\n",
    "* SitemapLoader\n",
    "| 사이트맵 기반으로 다수의 웹 페이지 로딩\n",
    "\n",
    "\n",
    "문서 플랫폼\n",
    "* NotionDBLoader\n",
    "| Notion의 DB에서 문서 불러오기 (API 필요)\n",
    "* ConfluenceLoader\n",
    "| Atlassian Confluence 문서 불러오기\n",
    "\n",
    "\n",
    "클라우드 문서\n",
    "* GoogleDriveLoader\n",
    "| 구글 드라이브에서 문서 불러오기\n",
    "* OneDriveLoader\n",
    "| 마이크로소프트 OneDrive에서 불러오기\n",
    "\n",
    "\n",
    "코드/로컬\n",
    "* GitLoader\n",
    "| Git 저장소 내 파일 불러오기\n",
    "* DirectoryLoader\n",
    "| 특정 폴더 내 모든 문서 일괄 불러오기\n",
    "\n",
    "\n",
    "메일\n",
    "* OutlookMessageLoader\n",
    "| Outlook .msg 파일 불러오기\n",
    "* EmailLoader\n",
    "| EML 또는 MIME 형식 이메일 로드\n",
    "\n",
    "\n",
    "이미지/스캔\n",
    "* UnstructuredImageLoader\n",
    "| 이미지에서 텍스트 추출 (OCR 기반)\n",
    "* UnstructuredPDFLoader\n",
    "| PDF 전처리 고급"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f91b4",
   "metadata": {},
   "source": [
    "#### 새로나온 텍스트 스플릿 기능 (추후 적용 해보기)\n",
    "\n",
    "* 유사도 기반 스플릿\n",
    "* https://python.langchain.com/api_reference/experimental/text_splitter.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474320af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 스플릿\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, \n",
    "                                            chunk_overlap=100)\n",
    "\n",
    "# 텍스트 파일을 load -> List[Document] 형태로 변환\n",
    "loader1 = TextLoader(\"..\\\\ref\\\\docs\\\\law_test.txt\")\n",
    "# 문서 분할\n",
    "split_doc1 = loader1.load_and_split(text_splitter)\n",
    "\n",
    "# 문서 개수 확인\n",
    "len(split_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f5af540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# 임시 DB 생성\n",
    "# db = Chroma.from_documents(\n",
    "#     documents=split_doc1, embedding=OpenAIEmbeddings(), \n",
    "#     collection_name=\"my_db\"\n",
    "# )\n",
    "\n",
    "# 저장할 경로 지정\n",
    "DB_PATH = \".\\\\chroma_db\"\n",
    "\n",
    "# # 문서를 디스크에 저장. 저장시 persist_directory에 저장할 경로를 지정.\n",
    "# persist_db = Chroma.from_documents(\n",
    "#     documents=split_doc1,\n",
    "#     embedding=embeddings, \n",
    "#     persist_directory=DB_PATH, \n",
    "#     collection_name=\"my_db2\"\n",
    "# )\n",
    "\n",
    "#디스크에서 문서를 로드.\n",
    "db = Chroma(\n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"my_db2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 데이터 확인\n",
    "persist_db.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabc961",
   "metadata": {},
   "source": [
    "#### 벡터 DB 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 추가하기\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# page_content, metadata, id 지정\n",
    "db.add_documents(\n",
    "    [\n",
    "        Document(\n",
    "            page_content=\"안녕하세요! 이번엔 도큐먼트를 새로 추가해 볼께요\",\n",
    "            metadata={\"source\": \"mydata.txt\"},\n",
    "            id=\"1\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b71409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 삭제하기\n",
    "# id 1 삭제\n",
    "db.delete(ids=[\"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d6996",
   "metadata": {},
   "source": [
    "#### 검색기 / Retrival 구현\n",
    "\n",
    "* 관련 문서는 다음을 참조\n",
    "  * https://python.langchain.com/docs/integrations/vectorstores/chroma/#initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147286c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmr 알고리즘 기반 retriever 만들기\n",
    "mmr_rt = db.as_retriever(\n",
    "  search_type=\"mmr\",\n",
    "  search_kwargs={\"k\": 5, \"lambda_mult\": 0.25, \"fetch_k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7692c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mmr_rt.invoke(\"주식 투자 세금에 대해 알려줘\"):\n",
    "    print(i.page_content)\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc560da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 기반 retriever 만들기\n",
    "sim_rt = db.as_retriever(\n",
    "  search_type=\"similarity_score_threshold\",\n",
    "  search_kwargs={\"k\": 3, \"score_threshold\": -0.5}\n",
    ")\n",
    "\n",
    "for i in sim_rt.invoke(\"세율 들에 대해 알려줘\"):\n",
    "    print(i.page_content)\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b96ec5",
   "metadata": {},
   "source": [
    "#### 멀티 모달 검색(추후 구현)\n",
    "\n",
    "* 여러 양식의 데이터를 포함하고 쿼리할 수 있는 컬렉션을 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cdc9c5",
   "metadata": {},
   "source": [
    "#### 이미지를 컬렉션에 포함하기 (이게 필요할까?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2ee7b",
   "metadata": {},
   "source": [
    "## LLM 체인 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5550fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",\n",
    "                 temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db0c7d7",
   "metadata": {},
   "source": [
    "스트림 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e910b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the following question based on the context: {context}\\nQuestion: {input}\"\n",
    ")\n",
    "\n",
    "chain = create_retrieval_chain(\n",
    "    retriever=mmr_rt,\n",
    "    combine_docs_chain=prompt | llm\n",
    ")\n",
    "\n",
    "# for chunk in chain.stream({\"input\": \"소득세법 적용 대상에 대해 알려줘\"}):\n",
    "#     print(chunk)\n",
    "\n",
    "for chunk in chain.stream({\"input\": \"소득세법 적용 대상에 대해 알려줘\"}):\n",
    "    # answer 키가 있을 때만 출력 (context 등은 무시)\n",
    "    if \"answer\" in chunk:\n",
    "        print(chunk[\"answer\"].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": \"소득세법 적용 대상에 대해 알려줘\"})\n",
    "# print(response[\"answer\"].content)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad147e",
   "metadata": {},
   "source": [
    "## RAG 평가 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d20ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions = 5\n",
    "documents = split_doc1  # 또는 loader1.load_and_split(text_splitter)\n",
    "llm_for_question = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.9)\n",
    "questions = []\n",
    "\n",
    "for i in range(num_questions):\n",
    "    prompt = f\"\"\"\n",
    "다음 문서를 바탕으로 질문을 1개 생성해 주세요.\n",
    "반드시 질문문장만 출력해 주세요. '질문:'이라는 표현 없이 완전한 한국어 질문 형태로만 작성해 주세요.\n",
    "\n",
    "문서 내용:\n",
    "{documents[i % len(documents)].page_content}\n",
    "\"\"\"\n",
    "    question = llm_for_question.invoke(prompt).content\n",
    "    questions.append(question)\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf368ff2",
   "metadata": {},
   "source": [
    "#### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,   # 생성된 답변이 원래 질문에 얼마나 적절한지 측정. 질문에 직접적으로 응답하는 답변은 높은 점수\n",
    "    context_precision,  # 생성된 답변에 사용된 컨텍스트 정보가 얼마나 관련성이 있는지 측정. 불필요한 정보 없이 질문과 관련된 컨텍스트만 포함되어 있으면 높은 점수\n",
    "    context_recall,     # 답변 생성에 필요한 모든 정보가 검색된 컨텍스트에 포함되어 있는지 측정. 질문에 답하는 데 필요한 정보가 모두 검색되면 높은 점수\n",
    "    answer_correctness  # 생성된 답변이 참조 답변과 비교하여 얼마나 정확한지 평가, 참조 답변이 필요\n",
    ")\n",
    "from ragas.evaluation import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_with_ragas(qa_chain, questions):\n",
    "    use_faithfulness = True\n",
    "    use_answer_relevancy = True\n",
    "    use_context_precision = True\n",
    "    use_context_recall = True    \n",
    "    use_answer_correctness = False\n",
    "        \n",
    "    evaluation_data = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": []\n",
    "    }\n",
    "    \n",
    "    if use_context_precision or use_context_recall:\n",
    "        evaluation_data[\"reference\"] = []\n",
    "    \n",
    "    for question in questions:\n",
    "        result = qa_chain.invoke({\"input\": question})\n",
    "        # 답변 추출 (AIMessage 객체의 content)\n",
    "        answer = result[\"answer\"].content if hasattr(result[\"answer\"], \"content\") else str(result[\"answer\"])\n",
    "        # context 추출 (Document 객체 리스트)\n",
    "        contexts = [doc.page_content for doc in result[\"context\"]]\n",
    "        \n",
    "        evaluation_data[\"question\"].append(question)\n",
    "        evaluation_data[\"answer\"].append(answer)\n",
    "        evaluation_data[\"contexts\"].append(contexts)\n",
    "        \n",
    "        if use_context_precision or use_context_recall:\n",
    "            evaluation_data[\"reference\"].append(contexts[0] if contexts else \"\")\n",
    "    \n",
    "    eval_dataset = Dataset.from_dict(evaluation_data)\n",
    "    \n",
    "    metrics = []\n",
    "    if use_faithfulness:\n",
    "        metrics.append(faithfulness)\n",
    "    if use_answer_relevancy:\n",
    "        metrics.append(answer_relevancy)\n",
    "    if use_context_precision:\n",
    "        metrics.append(context_precision)\n",
    "    if use_context_recall:\n",
    "        metrics.append(context_recall)\n",
    "    if use_answer_correctness:\n",
    "        metrics.append(answer_correctness)\n",
    "    \n",
    "    results = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"question\": evaluation_data[\"question\"],\n",
    "        \"answer\": evaluation_data[\"answer\"]\n",
    "    })\n",
    "    \n",
    "    for metric in metrics:\n",
    "        results_df[metric.name] = results[metric.name]\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_with_ragas(chain, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a376b3a",
   "metadata": {},
   "source": [
    "#### 평가결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"평가 결과:\")\n",
    "print(results)\n",
    "\n",
    "metric_columns = [col for col in results.columns if col not in [\"question\", \"answer\"]]\n",
    "if metric_columns:\n",
    "    avg_scores = results[metric_columns].mean()\n",
    "    print(\"\\n평균 점수:\")\n",
    "    print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd16dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install koreanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문별 지표 점수 막대그래프\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_ragas_results(results_df):\n",
    "    metric_columns = [col for col in results_df.columns if col not in [\"question\", \"answer\"]]\n",
    "    \n",
    "    for metric in metric_columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        sns.barplot(x=results_df.index, y=results_df[metric])\n",
    "        plt.title(f\"{metric} 점수 (질문별)\")\n",
    "        plt.xlabel(\"질문 번호\")\n",
    "        plt.ylabel(\"점수\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_ragas_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 평균값 레이다 차트\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_radar_chart(avg_scores):\n",
    "    metrics = avg_scores.index.tolist()\n",
    "    scores = avg_scores.values.tolist()\n",
    "    \n",
    "    # 각 축의 각도 설정 (점수를 먼저 안 늘림)\n",
    "    num_metrics = len(metrics)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()\n",
    "    \n",
    "    # 시작점으로 되돌아오기 위해 첫 점 추가 (마지막 값 = 첫 값)\n",
    "    scores += scores[:1]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    ax.plot(angles, scores, marker='o')\n",
    "    ax.fill(angles, scores, alpha=0.25)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_title(\"RAGAS 지표 평균 점수\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_radar_chart(avg_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53477e8",
   "metadata": {},
   "source": [
    "## 커스텀 평가지표 만드는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5284cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 평가 지표로 'hallucinations_metric'을 생성함\n",
    "# Faithfulness 점수를 기반으로 환각 정도를 계산 (1 - faithfulness)\n",
    "\n",
    "from dataclasses import dataclass, field  # 데이터 클래스 사용을 위한 모듈\n",
    "from ragas.metrics.base import MetricWithLLM, SingleTurnMetric, MetricType  # 평가 지표의 기본 클래스들\n",
    "from ragas.metrics import Faithfulness  # 기존의 Faithfulness 메트릭 사용\n",
    "import typing as t  # 타입 힌트를 위한 모듈\n",
    "from ragas.callbacks import Callbacks  # 평가 진행 중 이벤트 콜백\n",
    "from ragas.dataset_schema import SingleTurnSample  # 단일 질문-응답 샘플 형식 정의\n",
    "\n",
    "# 데이터 클래스 선언: 사용자 정의 평가 지표 HallucinationsMetric 정의\n",
    "@dataclass\n",
    "class HallucinationsMetric(MetricWithLLM, SingleTurnMetric):\n",
    "    # 지표 이름 설정\n",
    "    name: str = \"hallucinations_metric\"\n",
    "\n",
    "    # 이 지표를 계산하기 위해 필요한 데이터 열 정의 (사용자 질문, 응답, 검색된 문맥)\n",
    "    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n",
    "        default_factory=lambda: {\n",
    "            MetricType.SINGLE_TURN: {\"user_input\", \"response\", \"retrieved_contexts\"}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 클래스 초기화 이후 실행되는 후처리 함수\n",
    "    def __post_init__(self):\n",
    "        # 내부적으로 Faithfulness 메트릭을 초기화하여 활용함\n",
    "        self.faithfulness_metric = Faithfulness(llm=self.llm)\n",
    "\n",
    "    # 실제 점수를 계산하는 비동기 함수\n",
    "    async def _single_turn_ascore(\n",
    "        self, sample: SingleTurnSample, callbacks: Callbacks\n",
    "    ) -> float:\n",
    "        # Faithfulness 점수를 먼저 계산함\n",
    "        faithfulness_score = await self.faithfulness_metric.single_turn_ascore(\n",
    "            sample, callbacks\n",
    "        )\n",
    "        # 환각 점수 = 1 - faithfulness 점수\n",
    "        return 1 - faithfulness_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # 환경변수에서 API 키나 모델명 등을 불러오기 위해 사용\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper  # LangChain의 LLM을 RAGAS에서 사용 가능하도록 감싸주는 래퍼\n",
    "\n",
    "# OpenAI의 챗 모델을 LangChain에서 불러오기\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,  # 일관된 결과를 위해 생성 다양성 최소화\n",
    "    model_name=\"gpt-4o-mini\"  # 환경변수에서 모델 이름 불러오기 (예: \"gpt-4\")\n",
    ")\n",
    "\n",
    "# RAGAS에서 사용할 수 있도록 LangChain LLM을 래핑\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# 앞서 정의한 환각 평가 메트릭에 LLM을 연결하여 인스턴스 생성\n",
    "hallucinations_metric = HallucinationsMetric(llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 질문-응답 샘플을 생성하여 환각 점수를 계산하는 예시\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"파리는 어느 나라의 수도인가요?\",\n",
    "    response=\"파리는 프랑스의 수도입니다.\",\n",
    "    retrieved_contexts=[\"파리는 프랑스의 수도입니다. 프랑스는 유럽에 위치한 국가로, 파리는 세계적인 문화, 예술, 패션의 중심지로 알려져 있습니다.\"]\n",
    ")\n",
    "# 비동기 함수 실행 및 결과 출력\n",
    "score = await hallucinations_metric.single_turn_ascore(sample)\n",
    "print(\"Hallucination 점수:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12a80f",
   "metadata": {},
   "source": [
    "#### custom metric 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "# RAG 시스템(qa_chain)으로 생성된 답변과 그에 대한 질문·문맥 정보를 기반으로 RAGAS 평가 지표들(기본 + 사용자 정의 hallucinations)을 적용해 질문별 평가 결과를 데이터프레임 형태로 반환합니다.\n",
    "def evaluate_with_ragas_with_custom_metric(qa_chain, questions):\n",
    "    # 사용할 평가 지표 선택 (True: 사용 / False: 미사용)\n",
    "    use_faithfulness = True\n",
    "    use_answer_relevancy = True\n",
    "    use_context_precision = True\n",
    "    use_context_recall = True    \n",
    "    use_answer_correctness = False\n",
    "    use_hallucinations = True  # 사용자 정의 환각 평가 추가 여부\n",
    "\n",
    "    # 평가용 데이터 구조 초기화\n",
    "    evaluation_data = {\n",
    "        \"question\": [],   # 질문\n",
    "        \"answer\": [],     # 생성된 답변\n",
    "        \"contexts\": []    # 검색된 문서 조각 (복수)\n",
    "    }\n",
    "\n",
    "    # context_precision / recall을 위한 reference 컬럼 추가\n",
    "    if use_context_precision or use_context_recall:\n",
    "        evaluation_data[\"reference\"] = []\n",
    "\n",
    "    # 각 질문에 대해 RAG QA 체인 실행\n",
    "    for question in questions:\n",
    "        result = qa_chain.invoke({\"input\": question})  # RAG 시스템에서 답변 생성\n",
    "        answer = result[\"answer\"].content\n",
    "        contexts = [doc.page_content for doc in result[\"context\"]]\n",
    "\n",
    "        # 평가용 데이터 누적\n",
    "        evaluation_data[\"question\"].append(question)\n",
    "        evaluation_data[\"answer\"].append(answer)\n",
    "        evaluation_data[\"contexts\"].append(contexts)\n",
    "\n",
    "        # reference 문서 (보통 첫 번째 context) 추가\n",
    "        if use_context_precision or use_context_recall:\n",
    "            evaluation_data[\"reference\"].append(contexts[0] if contexts else \"\")\n",
    "\n",
    "    # 평가를 위한 huggingface Dataset 객체 생성\n",
    "    eval_dataset = Dataset.from_dict(evaluation_data)\n",
    "\n",
    "    # 사용할 메트릭 리스트 구성\n",
    "    metrics = []\n",
    "    if use_faithfulness:\n",
    "        metrics.append(faithfulness)\n",
    "    if use_answer_relevancy:\n",
    "        metrics.append(answer_relevancy)\n",
    "    if use_context_precision:\n",
    "        metrics.append(context_precision)\n",
    "    if use_context_recall:\n",
    "        metrics.append(context_recall)\n",
    "    if use_answer_correctness:\n",
    "        metrics.append(answer_correctness)\n",
    "    if use_hallucinations:\n",
    "        # 사용자 정의 환각 평가 메트릭 추가\n",
    "        hallucinations_metric = HallucinationsMetric(llm=evaluator_llm)\n",
    "        metrics.append(hallucinations_metric)\n",
    "\n",
    "    # RAGAS 평가 실행\n",
    "    results = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    # 결과를 DataFrame 형태로 정리\n",
    "    results_df = pd.DataFrame({\n",
    "        \"question\": evaluation_data[\"question\"],\n",
    "        \"answer\": evaluation_data[\"answer\"]\n",
    "    })\n",
    "\n",
    "    # 각 메트릭 결과 컬럼에 추가\n",
    "    for metric in metrics:\n",
    "        results_df[metric.name] = results[metric.name]\n",
    "\n",
    "    return results_df  # 평가 결과 데이터프레임 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_with_ragas_with_custom_metric(chain, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c674168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"평가 결과:\")\n",
    "print(results)\n",
    "\n",
    "metric_columns = [col for col in results.columns if col not in [\"question\", \"answer\"]]\n",
    "if metric_columns:\n",
    "    avg_scores = results[metric_columns].mean()\n",
    "    print(\"\\n평균 점수:\")\n",
    "    print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ragas_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
