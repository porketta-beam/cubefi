import streamlit as st
import os
import tempfile
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import koreanize_matplotlib
from datasets import Dataset

# Langchain imports
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

# RAGAS imports
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness
)
from ragas.evaluation import evaluate

# Load environment variables
from dotenv import load_dotenv
load_dotenv(dotenv_path='.env', override=True)

# Matplotlib í•œê¸€ ë° ìŒìˆ˜ ê¹¨ì§ ë°©ì§€ ì„¤ì •
plt.rcParams['axes.unicode_minus'] = False

# Set page config
st.set_page_config(
    page_title="RAG System with RAGAS Evaluation",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Initialize session state
if 'db' not in st.session_state:
    st.session_state.db = None
if 'documents' not in st.session_state:
    st.session_state.documents = None
if 'evaluation_results' not in st.session_state:
    st.session_state.evaluation_results = None
if 'generated_questions' not in st.session_state:
    st.session_state.generated_questions = []
if 'edited_questions' not in st.session_state:
    st.session_state.edited_questions = []
if 'chat_messages' not in st.session_state:
    st.session_state.chat_messages = []

st.title("ğŸ¤– RAG ì‹œìŠ¤í…œ ë° RAGAS í‰ê°€")
st.markdown("---")

# íƒ­ ì´ë¦„
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“š ë¬¸ì„œ ì„ë² ë”©", "ğŸ’¬ RAG í…ŒìŠ¤íŠ¸", "ğŸ“ ì§ˆë¬¸ ìƒì„±í•˜ê¸°", "ğŸ” RAG í‰ê°€í•˜ê¸°"])

with tab1:
    st.header("ë¬¸ì„œ ì„ë² ë”© ì„¤ì •")
    
    # File upload
    uploaded_file = st.file_uploader("í…ìŠ¤íŠ¸ ë˜ëŠ” PDF íŒŒì¼ ì—…ë¡œë“œ", type=['txt', 'pdf'])
    
    if uploaded_file is not None:
        # Save uploaded file temporarily
        file_extension = uploaded_file.name.split('.')[-1].lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_extension}') as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_file_path = tmp_file.name
        
        # Text splitter parameters
        st.subheader("í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •")
        col1, col2 = st.columns(2)
        
        with col1:
            chunk_size = st.slider("ì²­í¬ í¬ê¸°", min_value=100, max_value=2000, value=500, step=50)
        
        with col2:
            chunk_overlap = st.slider("ì²­í¬ ì¤‘ì²©", min_value=0, max_value=500, value=100, step=25)
        
        
        # Embedding model selection
        st.subheader("ì„ë² ë”© ì„¤ì •")
        embedding_model = st.selectbox(
            "Embedding Model",
            ["text-embedding-3-large", "text-embedding-3-small", "text-embedding-ada-002"],
            index=0
        )
        
        # Process documents button
        if st.button("ë¬¸ì„œ ì²˜ë¦¬", type="primary"):
            try:
                with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
                    # Load and split documents
                    if file_extension == 'txt':
                        loader = TextLoader(tmp_file_path)
                    elif file_extension == 'pdf':
                        loader = PyPDFLoader(tmp_file_path)
                    else:
                        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
                    
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    documents = loader.load_and_split(text_splitter)
                    
                    # Create embeddings
                    embeddings = OpenAIEmbeddings(model=embedding_model)
                    
                    # Create vector database
                    db = Chroma.from_documents(
                        documents=documents,
                        embedding=embeddings,
                        collection_name="rag_db"
                    )
                    
                    # Store in session state
                    st.session_state.db = db
                    st.session_state.documents = documents
                    
                    st.success(f"{len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # Display document info
                    st.subheader("ë¬¸ì„œ ì •ë³´")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ì´ ì²­í¬ ìˆ˜", len(documents))
                    
                    with col2:
                        avg_chunk_size = np.mean([len(doc.page_content) for doc in documents])
                        st.metric("í‰ê·  ì²­í¬ í¬ê¸°", f"{avg_chunk_size:.0f}ì")
                    
                    with col3:
                        total_chars = sum(len(doc.page_content) for doc in documents)
                        st.metric("ì´ ë¬¸ì ìˆ˜", f"{total_chars:,}")
                    
                    # Show sample chunks
                    st.subheader("ìƒ˜í”Œ ë¬¸ì„œ ì²­í¬")
                    for i, doc in enumerate(documents[:3]):
                        with st.expander(f"ì²­í¬ {i+1}"):
                            st.text(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)
                            
            except Exception as e:
                st.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # Clean up temporary file
        if os.path.exists(tmp_file_path):
            os.unlink(tmp_file_path)

with tab2:
    st.header("RAG í…ŒìŠ¤íŠ¸")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'ë¬¸ì„œ ì„ë² ë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ ì£¼ì„¸ìš”.")
    else:
        # Initialize chat messages
        if 'chat_messages' not in st.session_state:
            st.session_state.chat_messages = []
        
        # RAG Configuration
        st.subheader("RAG ì„¤ì •")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            chat_model = st.selectbox(
                "Answer Model",
                ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o", "gpt-4-turbo"],
                index=0,
                key="chat_model"
            )
        
        with col2:
            chat_temperature = st.slider("ë‹µë³€ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.0, step=0.1, key="chat_temp")
        
        with col3:
            search_k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜", min_value=1, max_value=10, value=3, key="chat_k")
        
        # Clear chat button
        if st.button("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.chat_messages = []
            st.rerun()
        
        # Display chat messages
        st.subheader("ì±„íŒ…")
        
        # Chat container
        chat_container = st.container()
        
        with chat_container:
            for i, message in enumerate(st.session_state.chat_messages):
                if message["role"] == "user":
                    with st.chat_message("user"):
                        st.write(message["content"])
                else:
                    with st.chat_message("assistant"):
                        st.write(message["content"])
                        if "contexts" in message:
                            with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                                for j, context in enumerate(message["contexts"]):
                                    st.write(f"**ë¬¸ì„œ {j+1}:**")
                                    st.write(context[:300] + "..." if len(context) > 300 else context)
                                    st.write("---")
        
        # Chat input
        user_question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
        
        if user_question:
            # Add user message to chat
            st.session_state.chat_messages.append({"role": "user", "content": user_question})
            
            try:
                with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    # Create retriever
                    retriever = st.session_state.db.as_retriever(
                        search_type="similarity",
                        search_kwargs={"k": search_k}
                    )
                    
                    # Create LLM instance
                    llm = ChatOpenAI(model=chat_model, temperature=chat_temperature)
                    
                    # Get relevant contexts
                    relevant_docs = retriever.invoke(user_question)
                    contexts = [doc.page_content for doc in relevant_docs]
                    
                    # Create prompt
                    context_text = "\n\n".join(contexts)
                    prompt_text = f"""ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ë§¥ ë‚´ì—ì„œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

ë¬¸ë§¥:
{context_text}

ì§ˆë¬¸: {user_question}

ë‹µë³€:"""
                    
                    # Get answer
                    response = llm.invoke(prompt_text)
                    answer = response.content
                    
                    # Add assistant message to chat
                    st.session_state.chat_messages.append({
                        "role": "assistant", 
                        "content": answer,
                        "contexts": contexts
                    })
                    
                    st.rerun()
                    
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # Add error message to chat
                st.session_state.chat_messages.append({
                    "role": "assistant", 
                    "content": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                })

with tab3:
    st.header("ì§ˆë¬¸ ìƒì„±í•˜ê¸°")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'ë¬¸ì„œ ì„ë² ë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ ì£¼ì„¸ìš”.")
    else:
        # LLM configuration for question generation
        st.subheader("LLM ì„¤ì •")
        
        col1, col2 = st.columns(2)
        
        with col1:
            question_model = st.selectbox(
                "Question Generation Model",
                ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4o", "gpt-4-turbo"],
                index=0
            )
        
        with col2:
            question_temperature = st.slider("ì§ˆë¬¸ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.9, step=0.1)
        
        # Number of questions
        num_questions = st.slider("ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜", min_value=1, max_value=10, value=5)
        
        # Generate questions
        if st.button("ì§ˆë¬¸ ìƒì„±", type="primary"):
            try:
                with st.spinner("ì§ˆë¬¸ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    # Create LLM instance
                    llm_for_question = ChatOpenAI(model_name=question_model, temperature=question_temperature)
                    
                    # Generate questions
                    questions = []
                    documents = st.session_state.documents
                    
                    for i in range(num_questions):
                        prompt_text = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì„ 1ê°œ ìƒì„±í•´ ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ ì§ˆë¬¸ë¬¸ì¥ë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”. 'ì§ˆë¬¸:'ì´ë¼ëŠ” í‘œí˜„ ì—†ì´ ì™„ì „í•œ í•œêµ­ì–´ ì§ˆë¬¸ í˜•íƒœë¡œë§Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{documents[i % len(documents)].page_content}
"""
                        question = llm_for_question.invoke(prompt_text).content
                        questions.append(question)
                    
                    # Store generated questions
                    st.session_state.generated_questions = questions
                    st.session_state.edited_questions = questions.copy()
                    
                    st.success(f"{len(questions)}ê°œì˜ ì§ˆë¬¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
            except Exception as e:
                st.error(f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # Display and edit questions
        if st.session_state.generated_questions:
            st.subheader("ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜ì •")
            st.write("ì•„ë˜ì—ì„œ ì§ˆë¬¸ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            
            # Edit questions
            for i, question in enumerate(st.session_state.generated_questions):
                edited_question = st.text_area(
                    f"ì§ˆë¬¸ {i+1}",
                    value=st.session_state.edited_questions[i] if i < len(st.session_state.edited_questions) else question,
                    key=f"question_{i}"
                )
                
                # Update edited questions in session state
                if len(st.session_state.edited_questions) <= i:
                    st.session_state.edited_questions.append(edited_question)
                else:
                    st.session_state.edited_questions[i] = edited_question
            
            # Add/Remove questions
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ì§ˆë¬¸ ì¶”ê°€"):
                    st.session_state.generated_questions.append("ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
                    st.session_state.edited_questions.append("ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
                    st.rerun()
            
            with col2:
                if st.button("ë§ˆì§€ë§‰ ì§ˆë¬¸ ì‚­ì œ") and len(st.session_state.generated_questions) > 1:
                    st.session_state.generated_questions.pop()
                    st.session_state.edited_questions.pop()
                    st.rerun()
            
            # Display final questions
            st.subheader("ìµœì¢… ì§ˆë¬¸ ëª©ë¡")
            for i, question in enumerate(st.session_state.edited_questions):
                st.write(f"{i+1}. {question}")

with tab4:
    st.header("RAG í‰ê°€í•˜ê¸°")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'ë¬¸ì„œ ì„ë² ë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ ì£¼ì„¸ìš”.")
    elif not st.session_state.edited_questions:
        st.warning("ë¨¼ì € 'ì§ˆë¬¸ ìƒì„±í•˜ê¸°' íƒ­ì—ì„œ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.")
    else:
        # Retriever configuration
        st.subheader("ê²€ìƒ‰ê¸° ì„¤ì •")
        
        col1, col2 = st.columns(2)
        
        with col1:
            search_type = st.selectbox(
                "Search Type",
                ["mmr", "similarity", "similarity_score_threshold"],
                index=0
            )
        
        with col2:
            k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (k)", min_value=1, max_value=20, value=5)
        
        # Additional parameters based on search type
        if search_type == "mmr":
            col1, col2 = st.columns(2)
            with col1:
                lambda_mult = st.slider("ëŒë‹¤ ê°€ì¤‘ì¹˜", min_value=0.0, max_value=1.0, value=0.25, step=0.05)
            with col2:
                fetch_k = st.slider("Fetch K (ê²€ìƒ‰ í›„ë³´ ìˆ˜)", min_value=k, max_value=50, value=10)
            
            search_kwargs = {"k": k, "lambda_mult": lambda_mult, "fetch_k": fetch_k}
        
        elif search_type == "similarity_score_threshold":
            score_threshold = st.slider("ì ìˆ˜ ì„ê³„ê°’", min_value=-1.0, max_value=1.0, value=0.5, step=0.1)
            search_kwargs = {"k": k, "score_threshold": score_threshold}
        
        else:
            search_kwargs = {"k": k}
        
        # LLM configuration
        st.subheader("LLM ì„¤ì •")
        
        answer_model = st.selectbox(
            "Answer Generation Model",
            ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o", "gpt-4-turbo"],
            index=0
        )
        answer_temperature = st.slider("ë‹µë³€ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.0, step=0.1)
        
        # RAGAS metrics selection
        st.subheader("RAGAS í‰ê°€ ì§€í‘œ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            use_faithfulness = st.checkbox("ì •í™•ì„±(Faithfulness)", value=True)
            use_answer_relevancy = st.checkbox("ë‹µë³€ ê´€ë ¨ì„±(Answer Relevancy)", value=True)
        
        with col2:
            use_context_precision = st.checkbox("ë¬¸ë§¥ ì •ë°€ë„(Context Precision)", value=True)
            use_context_recall = st.checkbox("ë¬¸ë§¥ ì¬í˜„ìœ¨(Context Recall)", value=True)
        
        # Display questions to be evaluated
        st.subheader("í‰ê°€í•  ì§ˆë¬¸ ëª©ë¡")
        for i, question in enumerate(st.session_state.edited_questions):
            st.write(f"{i+1}. {question}")
        
        # Evaluate questions
        if st.button("RAG í‰ê°€ ì‹¤í–‰", type="primary"):
            try:
                with st.spinner("ì§ˆë¬¸ì„ í‰ê°€ ì¤‘ì…ë‹ˆë‹¤..."):
                    # Create retriever
                    retriever = st.session_state.db.as_retriever(
                        search_type=search_type,
                        search_kwargs=search_kwargs
                    )
                    
                    # Create LLM instance
                    llm = ChatOpenAI(model=answer_model, temperature=answer_temperature)
                    
                    # Create chain
                    prompt = ChatPromptTemplate.from_template(
                        "Answer the following question based on the context: {context}\nQuestion: {input}"
                    )
                    chain = create_retrieval_chain(
                        retriever=retriever,
                        combine_docs_chain=prompt | llm
                    )
                    
                    # Evaluate with RAGAS
                    st.write("RAGASë¡œ í‰ê°€ ì¤‘...")
                    
                    evaluation_data = {
                        "question": [],
                        "answer": [],
                        "contexts": []
                    }
                    
                    if use_context_precision or use_context_recall:
                        evaluation_data["reference"] = []
                    
                    # Get answers for each question
                    for question in st.session_state.edited_questions:
                        result = chain.invoke({"input": question})
                        answer = result["answer"].content if hasattr(result["answer"], "content") else str(result["answer"])
                        contexts = [doc.page_content for doc in result["context"]]
                        
                        evaluation_data["question"].append(question)
                        evaluation_data["answer"].append(answer)
                        evaluation_data["contexts"].append(contexts)
                        
                        if use_context_precision or use_context_recall:
                            evaluation_data["reference"].append(contexts[0] if contexts else "")
                    
                    # Create evaluation dataset
                    eval_dataset = Dataset.from_dict(evaluation_data)
                    
                    # Select metrics
                    metrics = []
                    if use_faithfulness:
                        metrics.append(faithfulness)
                    if use_answer_relevancy:
                        metrics.append(answer_relevancy)
                    if use_context_precision:
                        metrics.append(context_precision)
                    if use_context_recall:
                        metrics.append(context_recall)
                    
                    # Evaluate
                    results = evaluate(eval_dataset, metrics=metrics)
                    
                    # Create results dataframe
                    results_df = pd.DataFrame({
                        "question": evaluation_data["question"],
                        "answer": evaluation_data["answer"]
                    })
                    
                    for metric in metrics:
                        results_df[metric.name] = results[metric.name]
                    
                    # Store results
                    st.session_state.evaluation_results = results_df
                    
                    st.success("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
            except Exception as e:
                st.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # Display results
        if st.session_state.evaluation_results is not None:
            st.subheader("í‰ê°€ ê²°ê³¼")
            
            results_df = st.session_state.evaluation_results
            
            # Display dataframe
            st.dataframe(results_df)
            
            # Calculate and display average scores
            metric_columns = [col for col in results_df.columns if col not in ["question", "answer"]]
            if metric_columns:
                avg_scores = results_df[metric_columns].mean()
                
                st.subheader("í‰ê·  ì ìˆ˜")
                col1, col2 = st.columns(2)
                
                with col1:
                    for i, (metric, score) in enumerate(avg_scores.items()):
                        if i < len(avg_scores) // 2 + len(avg_scores) % 2:
                            st.metric(metric.replace("_", " ").title(), f"{score:.3f}")
                
                with col2:
                    for i, (metric, score) in enumerate(avg_scores.items()):
                        if i >= len(avg_scores) // 2 + len(avg_scores) % 2:
                            st.metric(metric.replace("_", " ").title(), f"{score:.3f}")
                
                # Visualizations
                st.subheader("Visualizations")
                
                # Bar chart for each metric
                for metric in metric_columns:
                    fig, ax = plt.subplots(figsize=(10, 4))
                    sns.barplot(x=results_df.index, y=results_df[metric], ax=ax)
                    ax.set_title(f"ì§ˆë¬¸ë³„ {metric.replace('_', ' ').title()} ì ìˆ˜")
                    ax.set_xlabel("ì§ˆë¬¸ ë²ˆí˜¸")
                    ax.set_ylabel("ì ìˆ˜")
                    ax.set_ylim(0, 1)
                    plt.xticks(rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)
                
                # Radar chart for average scores
                if len(metric_columns) > 2:
                    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
                    
                    metrics = avg_scores.index.tolist()
                    scores = avg_scores.values.tolist()
                    
                    # Close the plot
                    scores += scores[:1]
                    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
                    angles += angles[:1]
                    
                    ax.plot(angles, scores, marker='o')
                    ax.fill(angles, scores, alpha=0.25)
                    ax.set_xticks(angles[:-1])
                    ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])
                    ax.set_ylim(0, 1)
                    ax.set_title("RAGAS í‰ê°€ ì§€í‘œ - í‰ê·  ì ìˆ˜")
                    
                    st.pyplot(fig)
            
            # Q&A Display
            st.subheader("ì§ˆë¬¸ê³¼ ë‹µë³€")
            for i, row in results_df.iterrows():
                with st.expander(f"Q{i+1}: {row['question']}"):
                    st.write("**ë‹µë³€:**")
                    st.write(row['answer'])
                    
                    if metric_columns:
                        st.write("**ì ìˆ˜:**")
                        for metric in metric_columns:
                            st.write(f"- {metric.replace('_', ' ').title()}: {row[metric]:.3f}")

# Add sidebar information
st.sidebar.title("â„¹ï¸ ì •ë³´")
st.sidebar.markdown("""
### ì‚¬ìš© ë°©ë²•:
1. **ë¬¸ì„œ ì„ë² ë”© íƒ­**: í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ ë¶„í•  íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
2. **RAG í…ŒìŠ¤íŠ¸ íƒ­**: RAG ì‹œìŠ¤í…œê³¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì±„íŒ…í•˜ë©° ë‹µë³€ì„ í™•ì¸í•˜ì„¸ìš”.
3. **ì§ˆë¬¸ ìƒì„±í•˜ê¸° íƒ­**: ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì„ ìë™ ìƒì„±í•˜ê³  í¸ì§‘í•˜ì„¸ìš”.
4. **RAG í‰ê°€í•˜ê¸° íƒ­**: ìƒì„±ëœ ì§ˆë¬¸ì„ RAG ì‹œìŠ¤í…œìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

### RAGAS í‰ê°€ ì§€í‘œ:
- **ì •í™•ì„±(Faithfulness)**: ë‹µë³€ì´ ì‚¬ì‹¤ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€
- **ë‹µë³€ ê´€ë ¨ì„±(Answer Relevancy)**: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€
- **ë¬¸ë§¥ ì •ë°€ë„(Context Precision)**: ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì–¼ë§ˆë‚˜ ì •ë°€í•œì§€
- **ë¬¸ë§¥ ì¬í˜„ìœ¨(Context Recall)**: ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì–¼ë§ˆë‚˜ ì™„ì „í•œì§€

### ìš”êµ¬ ì‚¬í•­:
- .env íŒŒì¼ì— OpenAI API í‚¤ í•„ìš”
- í•„ìˆ˜ Python íŒ¨í‚¤ì§€: streamlit, langchain, ragas ë“±
""")