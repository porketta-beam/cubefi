import streamlit as st
import os
import tempfile
import shutil
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import koreanize_matplotlib
from datasets import Dataset

# Langchain imports
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

# RAGAS imports
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness
)
from ragas.evaluation import evaluate

# Load environment variables
from dotenv import load_dotenv
load_dotenv(dotenv_path='.env', override=True)

# Matplotlib í•œê¸€ ë° ìŒìˆ˜ ê¹¨ì§ ë°©ì§€ ì„¤ì •
plt.rcParams['axes.unicode_minus'] = False

# Set page config
st.set_page_config(
    page_title="RAG System with RAGAS Evaluation",
    page_icon="ğŸ¤–",
    layout="wide"
)

# VectorDB ê´€ë¦¬ í•¨ìˆ˜ë“¤
def get_vectordb_path():
    """VectorDB ì €ì¥ ê²½ë¡œ ë°˜í™˜"""
    return "./chroma_db"

def check_vectordb_exists():
    """ë¡œì»¬ VectorDB ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    db_path = get_vectordb_path()
    return os.path.exists(db_path) and os.path.isdir(db_path) and len(os.listdir(db_path)) > 0

def load_existing_vectordb(embedding_model="text-embedding-3-large"):
    """ê¸°ì¡´ VectorDB ë¡œë“œ"""
    try:
        embeddings = OpenAIEmbeddings(model=embedding_model)
        db = Chroma(
            persist_directory=get_vectordb_path(),
            embedding_function=embeddings,
            collection_name="rag_db"
        )
        return db
    except Exception as e:
        st.error(f"VectorDB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def create_new_vectordb(documents, embedding_model="text-embedding-3-large"):
    """ìƒˆë¡œìš´ VectorDB ìƒì„± ë° ë¡œì»¬ ì €ì¥"""
    try:
        embeddings = OpenAIEmbeddings(model=embedding_model)
        
        # ê¸°ì¡´ ë””ë ‰í† ë¦¬ê°€ ìˆë‹¤ë©´ ì‚­ì œ
        db_path = get_vectordb_path()
        if os.path.exists(db_path):
            shutil.rmtree(db_path)
        
        # ìƒˆë¡œìš´ VectorDB ìƒì„±
        db = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            collection_name="rag_db",
            persist_directory=db_path
        )
        
        return db
    except Exception as e:
        st.error(f"VectorDB ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def add_documents_to_vectordb(new_documents, embedding_model="text-embedding-3-large"):
    """ê¸°ì¡´ VectorDBì— ìƒˆë¡œìš´ ë¬¸ì„œ ì¶”ê°€"""
    try:
        # ê¸°ì¡´ VectorDB ë¡œë“œ
        db = load_existing_vectordb(embedding_model)
        if not db:
            st.error("ê¸°ì¡´ VectorDBë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ìƒˆë¡œìš´ ë¬¸ì„œë“¤ì„ ê¸°ì¡´ ì»¬ë ‰ì…˜ì— ì¶”ê°€
        db.add_documents(new_documents)
        
        return db
    except Exception as e:
        st.error(f"ë¬¸ì„œ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def get_vectordb_documents(limit=None):
    """VectorDBì— ì €ì¥ëœ ë¬¸ì„œë“¤ ì¡°íšŒ"""
    try:
        db = load_existing_vectordb()
        if not db:
            return []
        
        # ChromaDBì—ì„œ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
        collection = db._collection
        
        # ëª¨ë“  ë¬¸ì„œì˜ IDì™€ ë©”íƒ€ë°ì´í„°, ë‚´ìš© ì¡°íšŒ
        if limit:
            results = collection.get(limit=limit, include=['documents', 'metadatas'])
        else:
            results = collection.get(include=['documents', 'metadatas'])
        
        documents_info = []
        for i, (doc_id, document, metadata) in enumerate(zip(results['ids'], results['documents'], results['metadatas'])):
            doc_info = {
                'id': doc_id,
                'content': document,
                'metadata': metadata or {},
                'content_preview': document[:200] + "..." if len(document) > 200 else document,
                'char_count': len(document)
            }
            documents_info.append(doc_info)
        
        return documents_info
    except Exception as e:
        st.error(f"ë¬¸ì„œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

def close_vectordb_connection():
    """VectorDB ì—°ê²° ì•ˆì „í•˜ê²Œ ì¢…ë£Œ"""
    import gc
    import time
    
    try:
        # session stateì˜ db ì—°ê²° í•´ì œ
        if 'db' in st.session_state and st.session_state.db is not None:
            db = st.session_state.db
            
            # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ChromaDB ì—°ê²° ì¢…ë£Œ ì‹œë„
            try:
                # ë°©ë²• 1: í´ë¼ì´ì–¸íŠ¸ ë¦¬ì…‹
                if hasattr(db, '_client') and db._client:
                    db._client.reset()
                    
                # ë°©ë²• 2: ì»¬ë ‰ì…˜ ì°¸ì¡° ì œê±°
                if hasattr(db, '_collection'):
                    db._collection = None
                    
                # ë°©ë²• 3: í´ë¼ì´ì–¸íŠ¸ ì°¸ì¡° ì œê±°
                if hasattr(db, '_client'):
                    db._client = None
                    
            except Exception as e:
                print(f"DB ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
            
            # session state ì´ˆê¸°í™”
            st.session_state.db = None
            del db  # ëª…ì‹œì  ì‚­ì œ
        
        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰
        for _ in range(3):
            gc.collect()
            time.sleep(0.3)
        
        # ë” ê¸´ ëŒ€ê¸° ì‹œê°„
        time.sleep(3.0)
        
        # ì¶”ê°€: ëª¨ë“  ì „ì—­ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì‹œë„
        try:
            import chromadb
            # ChromaDBì˜ ì „ì—­ í´ë¼ì´ì–¸íŠ¸ë“¤ ì •ë¦¬
            if hasattr(chromadb, '_client_cache'):
                chromadb._client_cache.clear()
        except:
            pass
        
        return True
    except Exception as e:
        print(f"ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def safe_delete_vectordb_with_retry(db_path, max_retries=3):
    """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•œ ì•ˆì „í•œ VectorDB ì‚­ì œ"""
    import time
    import gc
    
    for attempt in range(max_retries):
        try:
            # ì‚­ì œ ì‹œë„ ì „ ì¶”ê°€ ì •ë¦¬
            gc.collect()
            time.sleep(1.0)
            
            if os.path.exists(db_path):
                shutil.rmtree(db_path)
            return True
            
        except PermissionError as e:
            if attempt < max_retries - 1:
                st.warning(f"ì‚­ì œ ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨. {2 * (attempt + 1)}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(2 * (attempt + 1))  # ì ì§„ì ìœ¼ë¡œ ë” ì˜¤ë˜ ëŒ€ê¸°
            else:
                # ë§ˆì§€ë§‰ ì‹œë„ë„ ì‹¤íŒ¨í•œ ê²½ìš° ì´ë¦„ ë³€ê²½ìœ¼ë¡œ ëŒ€ì²´
                try:
                    backup_path = f"{db_path}_deleted_{int(time.time())}"
                    os.rename(db_path, backup_path)
                    st.warning(f"ì§ì ‘ ì‚­ì œ ì‹¤íŒ¨. í´ë”ë¥¼ {backup_path}ë¡œ ì´ë¦„ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œí•´ì£¼ì„¸ìš”.")
                    return True
                except Exception as rename_error:
                    raise e  # ì›ë˜ ì˜¤ë¥˜ ë‹¤ì‹œ ë°œìƒ
        except Exception as e:
            if attempt < max_retries - 1:
                st.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ. ì¬ì‹œë„ ì¤‘... ({str(e)})")
                time.sleep(2)
            else:
                raise e
    
    return False

def get_vectordb_info():
    """VectorDB ì •ë³´ ì¡°íšŒ"""
    if check_vectordb_exists():
        try:
            db = load_existing_vectordb()
            if db:
                collection = db._collection
                return {
                    "exists": True,
                    "count": collection.count(),
                    "path": get_vectordb_path()
                }
        except:
            pass
    
    return {
        "exists": False,
        "count": 0,
        "path": get_vectordb_path()
    }

# Initialize session state
if 'db' not in st.session_state:
    st.session_state.db = None
if 'documents' not in st.session_state:
    st.session_state.documents = None
if 'evaluation_results' not in st.session_state:
    st.session_state.evaluation_results = None
if 'generated_questions' not in st.session_state:
    st.session_state.generated_questions = []
if 'edited_questions' not in st.session_state:
    st.session_state.edited_questions = []
if 'chat_messages' not in st.session_state:
    st.session_state.chat_messages = []

st.title("ğŸ¤– RAG ì‹œìŠ¤í…œ ë° RAGAS í‰ê°€")
st.markdown("---")

# íƒ­ ì´ë¦„
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“š ë¬¸ì„œ ì„ë² ë”©", "ğŸ’¬ RAG í…ŒìŠ¤íŠ¸", "ğŸ“ ì§ˆë¬¸ ìƒì„±í•˜ê¸°", "ğŸ” RAG í‰ê°€í•˜ê¸°"])

with tab1:
    st.header("ë¬¸ì„œ ì„ë² ë”© ì„¤ì •")
    st.write("ìœˆë„ìš° ë³´ì•ˆ ì´ìŠˆ ë°œìƒ í¬ë¡œë§ˆì—ì„œ supabaseë¡œ ë³€ê²½ í•„ìš”")
    st.write("ì¼ë‹¨ ë…¸íŠ¸ë¶ íŒŒì¼ë¡œ db ì¶”ê°€í•  ê²ƒ")
    
    # VectorDB ìƒíƒœ í™•ì¸
    db_info = get_vectordb_info()
    
    st.subheader("ğŸ“ VectorDB ìƒíƒœ")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if db_info["exists"]:
            st.success("âœ… VectorDB ì¡´ì¬")
            st.metric("ì €ì¥ëœ ë¬¸ì„œ ìˆ˜", db_info["count"])
        else:
            st.warning("âŒ VectorDB ì—†ìŒ")
            st.metric("ì €ì¥ëœ ë¬¸ì„œ ìˆ˜", 0)
    
    with col2:
        st.info(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {db_info['path']}")
    
    with col3:
        if db_info["exists"]:
            if st.button("ğŸ—‘ï¸ VectorDB ì‚­ì œ", type="secondary"):
                try:
                    with st.spinner("VectorDB ì—°ê²°ì„ ì¢…ë£Œí•˜ê³  ì‚­ì œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                        # 1. ChromaDB ì—°ê²° ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
                        st.info("1ë‹¨ê³„: ChromaDB ì—°ê²° ì¢…ë£Œ ì¤‘...")
                        close_vectordb_connection()
                        
                        # 2. ë¬¸ì„œ ê´€ë ¨ session state ì´ˆê¸°í™”
                        st.info("2ë‹¨ê³„: ì„¸ì…˜ ë°ì´í„° ì´ˆê¸°í™” ì¤‘...")
                        st.session_state.documents = None
                        st.session_state.generated_questions = []
                        st.session_state.edited_questions = []
                        st.session_state.evaluation_results = None
                        
                        # 3. ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë””ë ‰í† ë¦¬ ì‚­ì œ
                        st.info("3ë‹¨ê³„: VectorDB íŒŒì¼ ì‚­ì œ ì¤‘...")
                        success = safe_delete_vectordb_with_retry(db_info['path'])
                        
                        if success:
                            st.success("âœ… VectorDBê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        else:
                            st.error("âŒ VectorDB ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            
                        st.rerun()
                        
                except Exception as e:
                    st.error(f"VectorDB ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.info("ğŸ’¡ í•´ê²° ë°©ë²•:")
                    st.info("1. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•œ í›„ ë‹¤ì‹œ ì‹œë„")
                    st.info("2. Streamlit ì„œë²„ ì¬ì‹œì‘ í›„ ì‹œë„")
                    st.info("3. ìˆ˜ë™ìœ¼ë¡œ ./chroma_db í´ë” ì‚­ì œ")
    
    # ê¸°ì¡´ VectorDB ê´€ë¦¬ ë²„íŠ¼ë“¤
    if db_info["exists"]:
        col1, col2 = st.columns(2)
        
        with col1:
            if st.session_state.db is None:
                if st.button("ğŸ“¥ ê¸°ì¡´ VectorDB ë¡œë“œ", type="primary"):
                    try:
                        with st.spinner("ê¸°ì¡´ VectorDBë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤..."):
                            # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’ ì‚¬ìš©)
                            embedding_model = "text-embedding-3-large"
                            db = load_existing_vectordb(embedding_model)
                            
                            if db:
                                st.session_state.db = db
                                st.success(f"ê¸°ì¡´ VectorDBë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤! (ë¬¸ì„œ ìˆ˜: {db_info['count']})")
                                st.rerun()
                            
                    except Exception as e:
                        st.error(f"VectorDB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        with col2:
            if st.button("ğŸ“‹ ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ë³´ê¸°"):
                with st.spinner("ë¬¸ì„œ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    documents = get_vectordb_documents(limit=100)  # ìµœëŒ€ 100ê°œê¹Œì§€ í‘œì‹œ
                    
                    if documents:
                        st.subheader("ğŸ“š ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡")
                        st.write(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        
                        # ë¬¸ì„œ ëª©ë¡ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œì‹œ
                        df_data = []
                        for i, doc in enumerate(documents):
                            source = doc['metadata'].get('source', 'Unknown')
                            page = doc['metadata'].get('page', 'N/A')
                            
                            df_data.append({
                                'No.': i + 1,
                                'Source': source.split('/')[-1] if source != 'Unknown' else source,  # íŒŒì¼ëª…ë§Œ í‘œì‹œ
                                'Page': page,
                                'Length': doc['char_count'],
                                'Preview': doc['content_preview']
                            })
                        
                        df = pd.DataFrame(df_data)
                        st.dataframe(df, use_container_width=True)
                        
                        # ìƒì„¸ ë‚´ìš© ë³´ê¸°
                        st.subheader("ğŸ“„ ë¬¸ì„œ ìƒì„¸ ë‚´ìš©")
                        selected_doc_idx = st.selectbox(
                            "ë¬¸ì„œ ì„ íƒ (ìƒì„¸ ë‚´ìš© ë³´ê¸°)",
                            range(len(documents)),
                            format_func=lambda x: f"ë¬¸ì„œ {x+1}: {documents[x]['metadata'].get('source', 'Unknown').split('/')[-1]} (í˜ì´ì§€ {documents[x]['metadata'].get('page', 'N/A')})"
                        )
                        
                        if selected_doc_idx is not None:
                            selected_doc = documents[selected_doc_idx]
                            
                            # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                            st.write("**ë©”íƒ€ë°ì´í„°:**")
                            metadata_cols = st.columns(3)
                            with metadata_cols[0]:
                                st.write(f"**Source:** {selected_doc['metadata'].get('source', 'Unknown')}")
                            with metadata_cols[1]:
                                st.write(f"**Page:** {selected_doc['metadata'].get('page', 'N/A')}")
                            with metadata_cols[2]:
                                st.write(f"**Length:** {selected_doc['char_count']} ì")
                            
                            # ë¬¸ì„œ ë‚´ìš© í‘œì‹œ
                            st.write("**ì „ì²´ ë‚´ìš©:**")
                            st.text_area(
                                "ë¬¸ì„œ ë‚´ìš©",
                                value=selected_doc['content'],
                                height=300,
                                key=f"doc_content_{selected_doc_idx}",
                                disabled=True
                            )
                    else:
                        st.warning("ì €ì¥ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # File upload
    st.subheader("ğŸ“„ ìƒˆë¡œìš´ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("í…ìŠ¤íŠ¸ ë˜ëŠ” PDF íŒŒì¼ ì—…ë¡œë“œ", type=['txt', 'pdf'])
    
    if uploaded_file is not None:
        # Save uploaded file temporarily
        file_extension = uploaded_file.name.split('.')[-1].lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_extension}') as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_file_path = tmp_file.name
        
        # Text splitter parameters
        st.subheader("í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •")
        col1, col2 = st.columns(2)
        
        with col1:
            chunk_size = st.slider("ì²­í¬ í¬ê¸°", min_value=100, max_value=2000, value=500, step=50)
        
        with col2:
            chunk_overlap = st.slider("ì²­í¬ ì¤‘ì²©", min_value=0, max_value=500, value=100, step=25)
        
        
        # Embedding model selection
        st.subheader("ì„ë² ë”© ì„¤ì •")
        embedding_model = st.selectbox(
            "Embedding Model",
            ["text-embedding-3-large", "text-embedding-3-small", "text-embedding-ada-002"],
            index=0
        )
        
        # VectorDB ì €ì¥ ì˜µì…˜
        st.subheader("VectorDB ì„¤ì •")
        
        # ê¸°ì¡´ DBê°€ ìˆëŠ” ê²½ìš° ì˜µì…˜ ì œê³µ
        if db_info["exists"]:
            operation_mode = st.radio(
                "ë¬¸ì„œ ì²˜ë¦¬ ë°©ì‹",
                options=["ìƒˆë¡œìš´ VectorDB ìƒì„± (ê¸°ì¡´ DB ë®ì–´ì“°ê¸°)", "ê¸°ì¡´ VectorDBì— ë¬¸ì„œ ì¶”ê°€"],
                index=1,
                help="ê¸°ì¡´ VectorDBê°€ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í• ì§€ ì„ íƒí•˜ì„¸ìš”."
            )
            save_to_local = True  # ê¸°ì¡´ DBê°€ ìˆìœ¼ë©´ í•­ìƒ ë¡œì»¬ ì €ì¥
        else:
            save_to_local = st.checkbox("ğŸ’¾ ë¡œì»¬ì— VectorDB ì €ì¥", value=True, help="ì²´í¬í•˜ë©´ VectorDBë¥¼ ë¡œì»¬ì— ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            operation_mode = "ìƒˆë¡œìš´ VectorDB ìƒì„±"
        
        # Process documents button
        if st.button("ë¬¸ì„œ ì²˜ë¦¬", type="primary"):
            try:
                with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
                    # Load and split documents
                    if file_extension == 'txt':
                        loader = TextLoader(tmp_file_path)
                    elif file_extension == 'pdf':
                        loader = PyPDFLoader(tmp_file_path)
                    else:
                        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
                    
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    documents = loader.load_and_split(text_splitter)
                    
                    # Create or update vector database
                    if operation_mode == "ê¸°ì¡´ VectorDBì— ë¬¸ì„œ ì¶”ê°€":
                        # ê¸°ì¡´ VectorDBì— ë¬¸ì„œ ì¶”ê°€
                        old_count = db_info["count"]
                        db = add_documents_to_vectordb(documents, embedding_model)
                        action_text = f"ê¸°ì¡´ VectorDBì— {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤! (ì´ {old_count + len(documents)}ê°œ)"
                    elif save_to_local:
                        # ë¡œì»¬ì— ì €ì¥í•˜ëŠ” ìƒˆë¡œìš´ VectorDB ìƒì„±
                        db = create_new_vectordb(documents, embedding_model)
                        action_text = f"{len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ê³  ë¡œì»¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!"
                    else:
                        # ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥í•˜ëŠ” VectorDB ìƒì„± (ê¸°ì¡´ ë°©ì‹)
                        embeddings = OpenAIEmbeddings(model=embedding_model)
                        db = Chroma.from_documents(
                            documents=documents,
                            embedding=embeddings,
                            collection_name="rag_db"
                        )
                        action_text = f"{len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!"
                    
                    if db:
                        # Store in session state
                        st.session_state.db = db
                        if operation_mode != "ê¸°ì¡´ VectorDBì— ë¬¸ì„œ ì¶”ê°€":
                            st.session_state.documents = documents
                        
                        st.success(action_text)
                        
                        # Display document info
                        st.subheader("ì²˜ë¦¬ëœ ë¬¸ì„œ ì •ë³´")
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            if operation_mode == "ê¸°ì¡´ VectorDBì— ë¬¸ì„œ ì¶”ê°€":
                                st.metric("ì¶”ê°€ëœ ì²­í¬ ìˆ˜", len(documents))
                                updated_db_info = get_vectordb_info()
                                st.metric("ì „ì²´ ì²­í¬ ìˆ˜", updated_db_info["count"])
                            else:
                                st.metric("ì´ ì²­í¬ ìˆ˜", len(documents))
                        
                        with col2:
                            avg_chunk_size = np.mean([len(doc.page_content) for doc in documents])
                            st.metric("í‰ê·  ì²­í¬ í¬ê¸°", f"{avg_chunk_size:.0f}ì")
                        
                        with col3:
                            total_chars = sum(len(doc.page_content) for doc in documents)
                            st.metric("ì²˜ë¦¬ëœ ë¬¸ì ìˆ˜", f"{total_chars:,}")
                        
                        # Show sample chunks
                        st.subheader("ìƒ˜í”Œ ë¬¸ì„œ ì²­í¬")
                        for i, doc in enumerate(documents[:3]):
                            with st.expander(f"ì²­í¬ {i+1}"):
                                st.text(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)
                    else:
                        st.error("VectorDB ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                                
            except Exception as e:
                st.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # Clean up temporary file
        if os.path.exists(tmp_file_path):
            os.unlink(tmp_file_path)

with tab2:
    st.header("RAG í…ŒìŠ¤íŠ¸")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'ë¬¸ì„œ ì„ë² ë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ ì£¼ì„¸ìš”.")
    else:
        # Initialize chat messages
        if 'chat_messages' not in st.session_state:
            st.session_state.chat_messages = []
        
        # RAG Configuration
        st.subheader("RAG ì„¤ì •")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            chat_model = st.selectbox(
                "Answer Model",
                ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o", "gpt-4-turbo"],
                index=0,
                key="chat_model"
            )
        
        with col2:
            chat_temperature = st.slider("ë‹µë³€ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.0, step=0.1, key="chat_temp")
        
        with col3:
            search_k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜", min_value=1, max_value=10, value=3, key="chat_k")
        
        # Clear chat button
        if st.button("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.chat_messages = []
            st.rerun()
        
        # Display chat messages
        st.subheader("ì±„íŒ…")
        
        # Chat container
        chat_container = st.container()
        
        with chat_container:
            for i, message in enumerate(st.session_state.chat_messages):
                if message["role"] == "user":
                    with st.chat_message("user"):
                        st.write(message["content"])
                else:
                    with st.chat_message("assistant"):
                        st.write(message["content"])
                        if "contexts" in message:
                            with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                                for j, context in enumerate(message["contexts"]):
                                    st.write(f"**ë¬¸ì„œ {j+1}:**")
                                    st.write(context[:300] + "..." if len(context) > 300 else context)
                                    st.write("---")
        
        # Chat input
        user_question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
        
        if user_question:
            # Add user message to chat
            st.session_state.chat_messages.append({"role": "user", "content": user_question})
            
            try:
                with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    # Create retriever
                    retriever = st.session_state.db.as_retriever(
                        search_type="similarity",
                        search_kwargs={"k": search_k}
                    )
                    
                    # Create LLM instance
                    llm = ChatOpenAI(model=chat_model, temperature=chat_temperature)
                    
                    # Get relevant contexts
                    relevant_docs = retriever.invoke(user_question)
                    contexts = [doc.page_content for doc in relevant_docs]
                    
                    # Create prompt
                    context_text = "\n\n".join(contexts)
                    prompt_text = f"""ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ë§¥ ë‚´ì—ì„œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

ë¬¸ë§¥:
{context_text}

ì§ˆë¬¸: {user_question}

ë‹µë³€:"""
                    
                    # Get answer
                    response = llm.invoke(prompt_text)
                    answer = response.content
                    
                    # Add assistant message to chat
                    st.session_state.chat_messages.append({
                        "role": "assistant", 
                        "content": answer,
                        "contexts": contexts
                    })
                    
                    st.rerun()
                    
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # Add error message to chat
                st.session_state.chat_messages.append({
                    "role": "assistant", 
                    "content": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                })

with tab3:
    st.header("ì§ˆë¬¸ ìƒì„±í•˜ê¸°")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'ë¬¸ì„œ ì„ë² ë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ ì£¼ì„¸ìš”.")
    else:
        # LLM configuration for question generation
        st.subheader("LLM ì„¤ì •")
        
        col1, col2 = st.columns(2)
        
        with col1:
            question_model = st.selectbox(
                "Question Generation Model",
                ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4o", "gpt-4-turbo"],
                index=0
            )
        
        with col2:
            question_temperature = st.slider("ì§ˆë¬¸ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.9, step=0.1)
        
        # Number of questions
        num_questions = st.slider("ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜", min_value=1, max_value=10, value=5)
        
        # Generate questions
        if st.button("ì§ˆë¬¸ ìƒì„±", type="primary"):
            try:
                with st.spinner("ì§ˆë¬¸ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    # Create LLM instance
                    llm_for_question = ChatOpenAI(model_name=question_model, temperature=question_temperature)
                    
                    # Generate questions
                    questions = []
                    documents = st.session_state.documents
                    
                    for i in range(num_questions):
                        prompt_text = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì„ 1ê°œ ìƒì„±í•´ ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ ì§ˆë¬¸ë¬¸ì¥ë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”. 'ì§ˆë¬¸:'ì´ë¼ëŠ” í‘œí˜„ ì—†ì´ ì™„ì „í•œ í•œêµ­ì–´ ì§ˆë¬¸ í˜•íƒœë¡œë§Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{documents[i % len(documents)].page_content}
"""
                        question = llm_for_question.invoke(prompt_text).content
                        questions.append(question)
                    
                    # Store generated questions
                    st.session_state.generated_questions = questions
                    st.session_state.edited_questions = questions.copy()
                    
                    st.success(f"{len(questions)}ê°œì˜ ì§ˆë¬¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
            except Exception as e:
                st.error(f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # Display and edit questions
        if st.session_state.generated_questions:
            st.subheader("ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜ì •")
            st.write("ì•„ë˜ì—ì„œ ì§ˆë¬¸ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            
            # Edit questions
            for i, question in enumerate(st.session_state.generated_questions):
                edited_question = st.text_area(
                    f"ì§ˆë¬¸ {i+1}",
                    value=st.session_state.edited_questions[i] if i < len(st.session_state.edited_questions) else question,
                    key=f"question_{i}"
                )
                
                # Update edited questions in session state
                if len(st.session_state.edited_questions) <= i:
                    st.session_state.edited_questions.append(edited_question)
                else:
                    st.session_state.edited_questions[i] = edited_question
            
            # Add/Remove questions
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ì§ˆë¬¸ ì¶”ê°€"):
                    st.session_state.generated_questions.append("ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
                    st.session_state.edited_questions.append("ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
                    st.rerun()
            
            with col2:
                if st.button("ë§ˆì§€ë§‰ ì§ˆë¬¸ ì‚­ì œ") and len(st.session_state.generated_questions) > 1:
                    st.session_state.generated_questions.pop()
                    st.session_state.edited_questions.pop()
                    st.rerun()
            
            # Display final questions
            st.subheader("ìµœì¢… ì§ˆë¬¸ ëª©ë¡")
            for i, question in enumerate(st.session_state.edited_questions):
                st.write(f"{i+1}. {question}")

with tab4:
    st.header("RAG í‰ê°€í•˜ê¸°")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'ë¬¸ì„œ ì„ë² ë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ ì£¼ì„¸ìš”.")
    elif not st.session_state.edited_questions:
        st.warning("ë¨¼ì € 'ì§ˆë¬¸ ìƒì„±í•˜ê¸°' íƒ­ì—ì„œ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.")
    else:
        # Retriever configuration
        st.subheader("ê²€ìƒ‰ê¸° ì„¤ì •")
        
        col1, col2 = st.columns(2)
        
        with col1:
            search_type = st.selectbox(
                "Search Type",
                ["mmr", "similarity", "similarity_score_threshold"],
                index=0
            )
        
        with col2:
            k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (k)", min_value=1, max_value=20, value=5)
        
        # Additional parameters based on search type
        if search_type == "mmr":
            col1, col2 = st.columns(2)
            with col1:
                lambda_mult = st.slider("ëŒë‹¤ ê°€ì¤‘ì¹˜", min_value=0.0, max_value=1.0, value=0.25, step=0.05)
            with col2:
                fetch_k = st.slider("Fetch K (ê²€ìƒ‰ í›„ë³´ ìˆ˜)", min_value=k, max_value=50, value=10)
            
            search_kwargs = {"k": k, "lambda_mult": lambda_mult, "fetch_k": fetch_k}
        
        elif search_type == "similarity_score_threshold":
            score_threshold = st.slider("ì ìˆ˜ ì„ê³„ê°’", min_value=-1.0, max_value=1.0, value=0.5, step=0.1)
            search_kwargs = {"k": k, "score_threshold": score_threshold}
        
        else:
            search_kwargs = {"k": k}
        
        # LLM configuration
        st.subheader("LLM ì„¤ì •")
        
        answer_model = st.selectbox(
            "Answer Generation Model",
            ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o", "gpt-4-turbo"],
            index=0
        )
        answer_temperature = st.slider("ë‹µë³€ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.0, step=0.1)
        
        # RAGAS metrics selection
        st.subheader("RAGAS í‰ê°€ ì§€í‘œ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            use_faithfulness = st.checkbox("ì •í™•ì„±(Faithfulness)", value=True)
            use_answer_relevancy = st.checkbox("ë‹µë³€ ê´€ë ¨ì„±(Answer Relevancy)", value=True)
        
        with col2:
            use_context_precision = st.checkbox("ë¬¸ë§¥ ì •ë°€ë„(Context Precision)", value=True)
            use_context_recall = st.checkbox("ë¬¸ë§¥ ì¬í˜„ìœ¨(Context Recall)", value=True)
        
        # Display questions to be evaluated
        st.subheader("í‰ê°€í•  ì§ˆë¬¸ ëª©ë¡")
        for i, question in enumerate(st.session_state.edited_questions):
            st.write(f"{i+1}. {question}")
        
        # Evaluate questions
        if st.button("RAG í‰ê°€ ì‹¤í–‰", type="primary"):
            try:
                with st.spinner("ì§ˆë¬¸ì„ í‰ê°€ ì¤‘ì…ë‹ˆë‹¤..."):
                    # Create retriever
                    retriever = st.session_state.db.as_retriever(
                        search_type=search_type,
                        search_kwargs=search_kwargs
                    )
                    
                    # Create LLM instance
                    llm = ChatOpenAI(model=answer_model, temperature=answer_temperature)
                    
                    # Create chain
                    prompt = ChatPromptTemplate.from_template(
                        "Answer the following question based on the context: {context}\nQuestion: {input}"
                    )
                    chain = create_retrieval_chain(
                        retriever=retriever,
                        combine_docs_chain=prompt | llm 
                    )
                    
                    # Evaluate with RAGAS
                    st.write("RAGASë¡œ í‰ê°€ ì¤‘...")
                    
                    evaluation_data = {
                        "question": [],
                        "answer": [],
                        "contexts": []
                    }
                    
                    if use_context_precision or use_context_recall:
                        evaluation_data["reference"] = []
                    
                    # Get answers for each question
                    for question in st.session_state.edited_questions:
                        result = chain.invoke({"input": question})
                        answer = result["answer"].content if hasattr(result["answer"], "content") else str(result["answer"])
                        contexts = [doc.page_content for doc in result["context"]]
                        
                        evaluation_data["question"].append(question)
                        evaluation_data["answer"].append(answer)
                        evaluation_data["contexts"].append(contexts)
                        
                        if use_context_precision or use_context_recall:
                            evaluation_data["reference"].append(contexts[0] if contexts else "")
                    
                    # Create evaluation dataset
                    eval_dataset = Dataset.from_dict(evaluation_data)
                    
                    # Select metrics
                    metrics = []
                    if use_faithfulness:
                        metrics.append(faithfulness)
                    if use_answer_relevancy:
                        metrics.append(answer_relevancy)
                    if use_context_precision:
                        metrics.append(context_precision)
                    if use_context_recall:
                        metrics.append(context_recall)
                    
                    # Evaluate
                    results = evaluate(eval_dataset, metrics=metrics)
                    
                    # Create results dataframe
                    results_df = pd.DataFrame({
                        "question": evaluation_data["question"],
                        "answer": evaluation_data["answer"]
                    })
                    
                    for metric in metrics:
                        results_df[metric.name] = results[metric.name]
                    
                    # Store results
                    st.session_state.evaluation_results = results_df
                    
                    st.success("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
            except Exception as e:
                st.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # Display results
        if st.session_state.evaluation_results is not None:
            st.subheader("í‰ê°€ ê²°ê³¼")
            
            results_df = st.session_state.evaluation_results
            
            # Display dataframe
            st.dataframe(results_df)
            
            # Calculate and display average scores
            metric_columns = [col for col in results_df.columns if col not in ["question", "answer"]]
            if metric_columns:
                avg_scores = results_df[metric_columns].mean()
                
                st.subheader("í‰ê·  ì ìˆ˜")
                col1, col2 = st.columns(2)
                
                with col1:
                    for i, (metric, score) in enumerate(avg_scores.items()):
                        if i < len(avg_scores) // 2 + len(avg_scores) % 2:
                            st.metric(metric.replace("_", " ").title(), f"{score:.3f}")
                
                with col2:
                    for i, (metric, score) in enumerate(avg_scores.items()):
                        if i >= len(avg_scores) // 2 + len(avg_scores) % 2:
                            st.metric(metric.replace("_", " ").title(), f"{score:.3f}")
                
                # Visualizations
                st.subheader("Visualizations")
                
                # Bar chart for each metric
                for metric in metric_columns:
                    fig, ax = plt.subplots(figsize=(10, 4))
                    sns.barplot(x=results_df.index, y=results_df[metric], ax=ax)
                    ax.set_title(f"ì§ˆë¬¸ë³„ {metric.replace('_', ' ').title()} ì ìˆ˜")
                    ax.set_xlabel("ì§ˆë¬¸ ë²ˆí˜¸")
                    ax.set_ylabel("ì ìˆ˜")
                    ax.set_ylim(0, 1)
                    plt.xticks(rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)
                
                # Radar chart for average scores
                if len(metric_columns) > 2:
                    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
                    
                    metrics = avg_scores.index.tolist()
                    scores = avg_scores.values.tolist()
                    
                    # Close the plot
                    scores += scores[:1]
                    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
                    angles += angles[:1]
                    
                    ax.plot(angles, scores, marker='o')
                    ax.fill(angles, scores, alpha=0.25)
                    ax.set_xticks(angles[:-1])
                    ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])
                    ax.set_ylim(0, 1)
                    ax.set_title("RAGAS í‰ê°€ ì§€í‘œ - í‰ê·  ì ìˆ˜")
                    
                    st.pyplot(fig)
            
            # Q&A Display
            st.subheader("ì§ˆë¬¸ê³¼ ë‹µë³€")
            for i, row in results_df.iterrows():
                with st.expander(f"Q{i+1}: {row['question']}"):
                    st.write("**ë‹µë³€:**")
                    st.write(row['answer'])
                    
                    if metric_columns:
                        st.write("**ì ìˆ˜:**")
                        for metric in metric_columns:
                            st.write(f"- {metric.replace('_', ' ').title()}: {row[metric]:.3f}")

# Add sidebar information
st.sidebar.title("â„¹ï¸ ì •ë³´")
st.sidebar.markdown("""
### ì‚¬ìš© ë°©ë²•:
1. **ë¬¸ì„œ ì„ë² ë”© íƒ­**: í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ ë¶„í•  íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
2. **RAG í…ŒìŠ¤íŠ¸ íƒ­**: RAG ì‹œìŠ¤í…œê³¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì±„íŒ…í•˜ë©° ë‹µë³€ì„ í™•ì¸í•˜ì„¸ìš”.
3. **ì§ˆë¬¸ ìƒì„±í•˜ê¸° íƒ­**: ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì„ ìë™ ìƒì„±í•˜ê³  í¸ì§‘í•˜ì„¸ìš”.
4. **RAG í‰ê°€í•˜ê¸° íƒ­**: ìƒì„±ëœ ì§ˆë¬¸ì„ RAG ì‹œìŠ¤í…œìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

### RAGAS í‰ê°€ ì§€í‘œ:
- **ì •í™•ì„±(Faithfulness)**: ë‹µë³€ì´ ì‚¬ì‹¤ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€
- **ë‹µë³€ ê´€ë ¨ì„±(Answer Relevancy)**: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€
- **ë¬¸ë§¥ ì •ë°€ë„(Context Precision)**: ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì–¼ë§ˆë‚˜ ì •ë°€í•œì§€
- **ë¬¸ë§¥ ì¬í˜„ìœ¨(Context Recall)**: ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì–¼ë§ˆë‚˜ ì™„ì „í•œì§€

### ìš”êµ¬ ì‚¬í•­:
- .env íŒŒì¼ì— OpenAI API í‚¤ í•„ìš”
- í•„ìˆ˜ Python íŒ¨í‚¤ì§€: streamlit, langchain, ragas ë“±
""")