"""Weight Optimization Experiment Module for Hybrid Search"""

import json
import os
import time
import numpy as np
import streamlit as st
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor
import pandas as pd


class WeightOptimizationExperiment:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ìµœì í™” ì‹¤í—˜ í´ë˜ìŠ¤"""
    
    def __init__(self, log_path: str = "./weight_search.log"):
        self.log_path = log_path
        self.log_dir = os.path.dirname(log_path) or "."
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir, exist_ok=True)
    
    def generate_weight_combinations(
        self, 
        min_weight: float = 0.0, 
        max_weight: float = 1.0, 
        step: float = 0.1
    ) -> List[Tuple[float, float]]:
        """ê°€ì¤‘ì¹˜ ì¡°í•© ìƒì„± (Grid Search)"""
        combinations = []
        
        # BM25ì™€ Vector ê°€ì¤‘ì¹˜ì˜ ëª¨ë“  ì¡°í•© ìƒì„±
        weights = np.arange(min_weight, max_weight + step, step)
        
        for bm25_weight in weights:
            for vector_weight in weights:
                # ê°€ì¤‘ì¹˜ í•©ì´ 1ì— ê°€ê¹ê±°ë‚˜ ë‘˜ ë‹¤ 0ì´ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
                total = bm25_weight + vector_weight
                if total > 0:
                    # ì •ê·œí™”
                    norm_bm25 = bm25_weight / total
                    norm_vector = vector_weight / total
                    combinations.append((
                        round(norm_bm25, 3), 
                        round(norm_vector, 3)
                    ))
        
        # ì¤‘ë³µ ì œê±°
        return list(set(combinations))
    
    def evaluate_search_quality(
        self, 
        hybrid_search_manager, 
        test_queries: List[str],
        bm25_weight: float, 
        vector_weight: float
    ) -> Dict[str, Any]:
        """íŠ¹ì • ê°€ì¤‘ì¹˜ ì¡°í•©ì˜ ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€"""
        
        total_score = 0.0
        query_scores = []
        total_latency = 0.0
        error_count = 0
        
        for query in test_queries:
            try:
                start_time = time.time()
                
                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
                results, info = hybrid_search_manager.search(
                    query=query,
                    search_type="hybrid",
                    k=5,  # ìƒìœ„ 5ê°œ ê²°ê³¼
                    weight_ratio=int(bm25_weight * 100)  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
                )
                
                latency = (time.time() - start_time) * 1000  # ms
                total_latency += latency
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì˜ˆì‹œ: ê²°ê³¼ ìˆ˜, í‰ê·  ì ìˆ˜, ë‹¤ì–‘ì„± ë“±)
                if results:
                    # ê¸°ë³¸ ì ìˆ˜: ê²°ê³¼ ê°œìˆ˜ì™€ ì ìˆ˜ì˜ ì¡°í™”í‰ê· 
                    result_count_score = min(len(results) / 5.0, 1.0)  # ìµœëŒ€ 1.0
                    
                    # í‰ê·  ì ìˆ˜ (ì •ê·œí™”)
                    if hasattr(results[0], 'score') or (isinstance(results[0], dict) and 'score' in results[0]):
                        scores = [r.score if hasattr(r, 'score') else r['score'] for r in results]
                        avg_score = np.mean(scores) if scores else 0.0
                        avg_score_normalized = min(avg_score / 10.0, 1.0)  # 10ìœ¼ë¡œ ì •ê·œí™”
                    else:
                        avg_score_normalized = 0.5  # ê¸°ë³¸ê°’
                    
                    # ê²°ê³¼ ë‹¤ì–‘ì„± (íŒŒì¼ëª… ê¸°ì¤€)
                    if len(results) > 1:
                        filenames = set()
                        for r in results:
                            if hasattr(r, 'filename'):
                                filenames.add(r.filename)
                            elif isinstance(r, dict) and 'filename' in r:
                                filenames.add(r['filename'])
                        diversity_score = len(filenames) / len(results)
                    else:
                        diversity_score = 1.0 if results else 0.0
                    
                    # ì¢…í•© ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
                    query_score = (
                        result_count_score * 0.4 + 
                        avg_score_normalized * 0.4 + 
                        diversity_score * 0.2
                    )
                else:
                    query_score = 0.0
                
                query_scores.append(query_score)
                total_score += query_score
                
            except Exception as e:
                error_count += 1
                st.warning(f"ì¿¼ë¦¬ '{query}' í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                query_scores.append(0.0)
        
        # ì „ì²´ í‰ê°€ ì§€í‘œ ê³„ì‚°
        num_queries = len(test_queries)
        avg_score = total_score / num_queries if num_queries > 0 else 0.0
        avg_latency = total_latency / num_queries if num_queries > 0 else 0.0
        error_rate = error_count / num_queries if num_queries > 0 else 0.0
        
        # í‘œì¤€í¸ì°¨ ê³„ì‚° (ì¼ê´€ì„± ì§€í‘œ)
        score_std = np.std(query_scores) if query_scores else 0.0
        
        return {
            "avg_score": round(avg_score, 4),
            "avg_latency_ms": round(avg_latency, 2),
            "error_rate": round(error_rate, 4),
            "score_std": round(score_std, 4),
            "query_scores": query_scores,
            "total_queries": num_queries
        }
    
    def run_weight_experiment(
        self, 
        hybrid_search_manager,
        test_queries: List[str],
        weight_combinations: Optional[List[Tuple[float, float]]] = None,
        max_workers: int = 3
    ) -> str:
        """ê°€ì¤‘ì¹˜ ìµœì í™” ì‹¤í—˜ ì‹¤í–‰"""
        
        if weight_combinations is None:
            weight_combinations = self.generate_weight_combinations(
                min_weight=0.0, max_weight=1.0, step=0.1
            )
        
        st.info(f"ğŸ”¬ {len(weight_combinations)}ê°œ ê°€ì¤‘ì¹˜ ì¡°í•©ìœ¼ë¡œ ì‹¤í—˜ ì‹œì‘")
        st.info(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜: {len(test_queries)}ê°œ")
        
        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        results = []
        experiment_start_time = datetime.now()
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor ì‚¬ìš©
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ëª¨ë“  ì‹¤í—˜ì„ ìŠ¤ì¼€ì¤„ë§
            future_to_weights = {
                executor.submit(
                    self.evaluate_search_quality,
                    hybrid_search_manager,
                    test_queries,
                    bm25_weight,
                    vector_weight
                ): (bm25_weight, vector_weight)
                for bm25_weight, vector_weight in weight_combinations
            }
            
            completed = 0
            total_experiments = len(weight_combinations)
            
            # ì™„ë£Œëœ ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘
            for future in future_to_weights:
                try:
                    bm25_weight, vector_weight = future_to_weights[future]
                    evaluation_result = future.result(timeout=60)  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
                    
                    # ë¡œê·¸ ì—”íŠ¸ë¦¬ ìƒì„±
                    log_entry = {
                        "timestamp": datetime.now().isoformat(),
                        "experiment_id": f"exp_{experiment_start_time.strftime('%Y%m%d_%H%M%S')}",
                        "bm25_weight": bm25_weight,
                        "vector_weight": vector_weight,
                        "weights": {
                            "bm25": bm25_weight,
                            "vector": vector_weight
                        },
                        "score": evaluation_result["avg_score"],
                        "metrics": evaluation_result,
                        "test_queries": test_queries
                    }
                    
                    results.append(log_entry)
                    
                    # JSON Lines í˜•ì‹ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ì— ì¦‰ì‹œ ì €ì¥
                    with open(self.log_path, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
                    
                    completed += 1
                    progress = completed / total_experiments
                    progress_bar.progress(progress)
                    status_text.text(
                        f"ì§„í–‰ë¥ : {completed}/{total_experiments} "
                        f"({progress*100:.1f}%) | "
                        f"í˜„ì¬: BM25={bm25_weight:.2f}, Vector={vector_weight:.2f} | "
                        f"ì ìˆ˜: {evaluation_result['avg_score']:.4f}"
                    )
                    
                except Exception as e:
                    completed += 1
                    progress = completed / total_experiments
                    progress_bar.progress(progress)
                    st.error(f"ì‹¤í—˜ ì‹¤íŒ¨ - BM25: {bm25_weight}, Vector: {vector_weight}: {str(e)}")
        
        # ì‹¤í—˜ ì™„ë£Œ ë©”ì‹œì§€
        experiment_duration = (datetime.now() - experiment_start_time).total_seconds()
        st.success(f"âœ… ì‹¤í—˜ ì™„ë£Œ! ì´ {len(results)}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {experiment_duration:.1f}ì´ˆ)")
        st.success(f"ğŸ“ ê²°ê³¼ê°€ {self.log_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return self.log_path
    
    def load_experiment_results(self) -> pd.DataFrame:
        """ì €ì¥ëœ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
        if not os.path.exists(self.log_path):
            return pd.DataFrame()
        
        results = []
        try:
            with open(self.log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        results.append(data)
            
            return pd.DataFrame(results)
        except Exception as e:
            st.error(f"ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()
    
    def get_default_test_queries(self) -> List[str]:
        """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¸íŠ¸"""
        return [
            "ì†Œë“ì„¸ ê³µì œ ë°©ë²•",
            "ë¶€ê°€ê°€ì¹˜ì„¸ ì‹ ê³  ê¸°í•œ", 
            "ì‚¬ì—…ì ì„¸ê¸ˆ ê³„ì‚°",
            "ì—°ë§ì •ì‚° ê³µì œ í•­ëª©",
            "ì›ì²œì§•ìˆ˜ ì„¸ìœ¨",
            "ë²•ì¸ì„¸ ê³„ì‚° ë°©ë²•",
            "ì–‘ë„ì†Œë“ì„¸ ë©´ì œ ì¡°ê±´",
            "ìƒì†ì„¸ ê³µì œ í•œë„",
            "ì¢…í•©ë¶€ë™ì‚°ì„¸ ê³„ì‚°",
            "ê·¼ë¡œì†Œë“ì„¸ êµ¬ê°„"
        ]
    
    def clear_experiment_log(self) -> bool:
        """ì‹¤í—˜ ë¡œê·¸ íŒŒì¼ ì‚­ì œ"""
        try:
            if os.path.exists(self.log_path):
                os.remove(self.log_path)
                return True
            return True
        except Exception as e:
            st.error(f"ë¡œê·¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            return False


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_weight_experiment = None

def get_weight_experiment(log_path: str = "./weight_search.log") -> WeightOptimizationExperiment:
    """Weight Optimization Experiment ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _weight_experiment
    if _weight_experiment is None:
        _weight_experiment = WeightOptimizationExperiment(log_path)
    return _weight_experiment