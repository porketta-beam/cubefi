import streamlit as st
import os
import shutil
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import koreanize_matplotlib
from datasets import Dataset

# Langchain imports
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

# RAGAS imports
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from ragas.evaluation import evaluate

# Load environment variables
from dotenv import load_dotenv
load_dotenv(dotenv_path='.env', override=True)

from typing import List, Dict, Any
from mod import (
    ChromaDBManager,
    RawDataSyncManager,
    RAGSystemManager,
    QuestionGenerationManager,
    EvaluationManager,
    ChatInterfaceManager,
    VisualizationUtils
)

# Matplotlib í•œê¸€ ë° ìŒìˆ˜ ê¹¨ì§ ë°©ì§€ ì„¤ì •
plt.rcParams['axes.unicode_minus'] = False

# Set page config
st.set_page_config(
    page_title="RAG System with RAGAS Evaluation",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Document loading and splitting function
def load_and_split_document(file_path: str, chunk_size: int = 500, chunk_overlap: int = 100) -> list:
    """íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• """
    try:
        file_extension = Path(file_path).suffix.lower()
        
        # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ë¡œë” ì„ íƒ
        if file_extension == '.txt':
            loader = TextLoader(file_path, encoding='utf-8')
        elif file_extension == '.pdf':
            loader = PyPDFLoader(file_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
        documents = loader.load_and_split(text_splitter)
        
        return documents
        
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({os.path.basename(file_path)}): {str(e)}")
        return []


# Initialize session state
if 'db' not in st.session_state:
    st.session_state.db = None
if 'documents' not in st.session_state:
    st.session_state.documents = None
if 'evaluation_results' not in st.session_state:
    st.session_state.evaluation_results = None
if 'generated_questions' not in st.session_state:
    st.session_state.generated_questions = []
if 'edited_questions' not in st.session_state:
    st.session_state.edited_questions = []
if 'chat_messages' not in st.session_state:
    st.session_state.chat_messages = []
if 'db_manager' not in st.session_state:
    st.session_state.db_manager = ChromaDBManager()
if 'sync_manager' not in st.session_state:
    st.session_state.sync_manager = RawDataSyncManager()
if 'rag_manager' not in st.session_state:
    st.session_state.rag_manager = RAGSystemManager()
if 'question_manager' not in st.session_state:
    st.session_state.question_manager = QuestionGenerationManager()
if 'evaluation_manager' not in st.session_state:
    st.session_state.evaluation_manager = EvaluationManager()
if 'chat_interface' not in st.session_state:
    st.session_state.chat_interface = ChatInterfaceManager()

st.title("ğŸ¤– RAG ì‹œìŠ¤í…œ ë° RAGAS í‰ê°€")
st.markdown("---")

# íƒ­ ì´ë¦„
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”„ Raw Data ë™ê¸°í™”", "ğŸ’¬ RAG í…ŒìŠ¤íŠ¸", "ğŸ“ ì§ˆë¬¸ ìƒì„±í•˜ê¸°", "ğŸ” RAG í‰ê°€í•˜ê¸°"])

with tab1:
    st.header("Raw Data ë™ê¸°í™”")
    
    # ë™ê¸°í™” ê´€ë¦¬ì ê°€ì ¸ì˜¤ê¸°
    sync_manager = st.session_state.sync_manager
    db_manager = st.session_state.db_manager
    
    # VectorDB ìƒíƒœ í‘œì‹œ
    db_status = db_manager.get_status()
    
    st.subheader("ğŸ“ VectorDB ìƒíƒœ")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if db_status["db_exists"]:
            st.success("âœ… VectorDB ì¡´ì¬")
            st.metric("ì €ì¥ëœ ë¬¸ì„œ ìˆ˜", db_status["document_count"])
        else:
            st.warning("âŒ VectorDB ì—†ìŒ")
            st.metric("ì €ì¥ëœ ë¬¸ì„œ ìˆ˜", 0)
    
    with col2:
        st.info(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {db_status['db_path']}")
        st.info(f"ğŸ”¤ ì„ë² ë”© ëª¨ë¸: {db_status['embedding_model']}")
    
    with col3:
        if db_status["db_exists"]:
            if st.button("ğŸ—‘ï¸ VectorDB ì‚­ì œ", type="secondary"):
                try:
                    with st.spinner("VectorDBë¥¼ ì‚­ì œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                        # ì„¸ì…˜ ë°ì´í„° ì´ˆê¸°í™”
                        st.session_state.documents = None
                        st.session_state.generated_questions = []
                        st.session_state.edited_questions = []
                        st.session_state.evaluation_results = None
                        st.session_state.db = None
                        
                        # DB ì‚­ì œ
                        success = db_manager.delete_db()
                        
                        if success:
                            st.success("âœ… VectorDBê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        else:
                            st.error("âŒ VectorDB ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            
                        st.rerun()
                        
                except Exception as e:
                    st.error(f"VectorDB ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    st.markdown("---")
    
    st.subheader("ğŸ“Š ë™ê¸°í™” ìƒíƒœ")
    
    # ë™ê¸°í™” ìƒíƒœ í™•ì¸
    if st.button("ğŸ”„ ë™ê¸°í™” ìƒíƒœ í™•ì¸"):
        sync_status = sync_manager.compare_with_db(db_manager)
        raw_files_info = sync_manager.scan_raw_data_folder()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("ğŸ“ **Raw Data í´ë” ìƒíƒœ**")
            st.write(f"- ê²½ë¡œ: {sync_manager.raw_data_path}")
            st.write(f"- ì´ íŒŒì¼ ìˆ˜: {len(raw_files_info)}ê°œ")
            
            if raw_files_info:
                total_size_mb = sum(info['size_mb'] for info in raw_files_info)
                st.write(f"- ì´ íŒŒì¼ í¬ê¸°: {total_size_mb:.2f} MB")
                
                # íŒŒì¼ ëª©ë¡ í‘œì‹œ
                st.write("**íŒŒì¼ ëª©ë¡:**")
                for file_info in raw_files_info:
                    st.write(f"  - {file_info['filename']} ({file_info['size_mb']:.2f} MB)")
        
        with col2:
            st.write("ğŸ—„ï¸ **ChromaDB ìƒíƒœ**")
            db_status = db_manager.get_status()
            st.write(f"- DB ì¡´ì¬: {'âœ…' if db_status['db_exists'] else 'âŒ'}")
            st.write(f"- ì´ ë¬¸ì„œ ìˆ˜: {db_status['document_count']}ê°œ")
            st.write(f"- ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len(sync_status['all_db_files'])}ê°œ")
            
            # ë™ê¸°í™” ìƒíƒœ
            st.write("**ë™ê¸°í™” ìƒíƒœ:**")
            st.write(f"- ìƒˆ íŒŒì¼: {len(sync_status['new_files'])}ê°œ")
            st.write(f"- ë™ê¸°í™”ë¨: {len(sync_status['existing_files'])}ê°œ")
            st.write(f"- ê³ ì•„ íŒŒì¼: {len(sync_status['orphaned_files'])}ê°œ")
            
            # ìƒì„¸ ì •ë³´
            if sync_status['new_files']:
                st.write("**ì¶”ê°€í•  ìƒˆ íŒŒì¼:**")
                for filename in sync_status['new_files']:
                    st.write(f"  - {filename}")
            
            if sync_status['orphaned_files']:
                st.write("**ê³ ì•„ íŒŒì¼ (raw_dataì— ì—†ìŒ):**")
                for filename in sync_status['orphaned_files']:
                    st.write(f"  - {filename}")
    
    st.markdown("---")
    
    # ë™ê¸°í™” ì„¤ì •
    st.subheader("âš™ï¸ ë™ê¸°í™” ì„¤ì •")
    
    col1, col2 = st.columns(2)
    
    with col1:
        sync_chunk_size = st.slider("ì²­í¬ í¬ê¸°", min_value=100, max_value=2000, value=500, step=50, key="sync_chunk_size")
    
    with col2:
        sync_chunk_overlap = st.slider("ì²­í¬ ì¤‘ì²©", min_value=0, max_value=500, value=100, step=25, key="sync_chunk_overlap")
    
    # ë™ê¸°í™” ì‹¤í–‰
    if st.button("ğŸš€ ë™ê¸°í™” ì‹¤í–‰", type="primary"):
        with st.spinner("ë™ê¸°í™”ë¥¼ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
            success = sync_manager.sync_with_db(
                db_manager=db_manager,
                chunk_size=sync_chunk_size,
                chunk_overlap=sync_chunk_overlap
            )
            
            if success:
                # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.db = db_manager.db
                st.rerun()
            else:
                st.error("ë™ê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # ì¶”ê°€ ì •ë³´
    st.info("ğŸ’¡ **ì‚¬ìš©ë²•:** raw_data í´ë”ì— .txt ë˜ëŠ” .pdf íŒŒì¼ì„ ì¶”ê°€í•œ í›„ ë™ê¸°í™”ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    st.info(f"ğŸ“‚ **Raw Data í´ë” ê²½ë¡œ:** {sync_manager.raw_data_path}")
    
    # ê¸°ì¡´ DB ê´€ë¦¬ ê¸°ëŠ¥
    if db_status["db_exists"]:
        st.markdown("---")
        st.subheader("ğŸ“‹ VectorDB ê´€ë¦¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if not db_status["db_loaded"]:
                if st.button("ğŸ“¥ ê¸°ì¡´ VectorDB ë¡œë“œ", type="primary"):
                    try:
                        with st.spinner("ê¸°ì¡´ VectorDBë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤..."):
                            success = db_manager.load_existing_db()
                            
                            if success:
                                st.session_state.db = db_manager.db
                                st.success(f"ê¸°ì¡´ VectorDBë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤! (ë¬¸ì„œ ìˆ˜: {db_status['document_count']})")
                                st.rerun()
                            
                    except Exception as e:
                        st.error(f"VectorDB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        with col2:
            if st.button("ğŸ“‹ ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ë³´ê¸°"):
                with st.spinner("ë¬¸ì„œ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    files_in_db = db_manager.get_files_in_db()
                    
                    if files_in_db:
                        st.subheader("ğŸ“š ì €ì¥ëœ íŒŒì¼ ëª©ë¡")
                        st.write(f"ì´ {len(files_in_db)}ê°œì˜ íŒŒì¼ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        st.write(f"ì´ ë¬¸ì„œ ì²­í¬ ìˆ˜: {db_status['document_count']}ê°œ")
                        
                        # íŒŒì¼ ëª©ë¡ í‘œì‹œ
                        for i, filename in enumerate(files_in_db, 1):
                            st.write(f"{i}. {filename}")
                    else:
                        st.warning("ì €ì¥ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with tab2:
    st.header("RAG í…ŒìŠ¤íŠ¸")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'Raw Data ë™ê¸°í™”' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë™ê¸°í™”í•´ ì£¼ì„¸ìš”.")
    else:
        # RAG Configuration
        st.subheader("RAG ì„¤ì •")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            chat_model = st.selectbox(
                "Answer Model",
                ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o", "gpt-4-turbo"],
                index=0,
                key="chat_model"
            )
        
        with col2:
            chat_temperature = st.slider("ë‹µë³€ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.0, step=0.1, key="chat_temp")
        
        with col3:
            search_k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜", min_value=1, max_value=10, value=3, key="chat_k")
        
        # RAG ë§¤ë‹ˆì € ì„¤ì •
        rag_manager = st.session_state.rag_manager
        rag_manager.set_llm(chat_model, chat_temperature)
        rag_manager.set_retriever(st.session_state.db, "similarity", {"k": search_k})
        
        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë§¤ë‹ˆì €
        chat_interface = st.session_state.chat_interface
        
        # Clear chat button
        if st.button("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"):
            chat_interface.clear_messages()
            st.rerun()
        
        # Display chat messages
        st.subheader("ì±„íŒ…")
        
        # Chat container
        chat_container = st.container()
        
        with chat_container:
            chat_interface.display_messages()
        
        # Chat input
        user_question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
        
        if user_question:
            # Add user message to chat
            chat_interface.add_message("user", user_question)
            
            try:
                with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    # Get answer using RAG manager
                    answer, contexts = rag_manager.get_answer(user_question)
                    
                    # Add assistant message to chat
                    chat_interface.add_message("assistant", answer, contexts)
                    
                    st.rerun()
                    
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # Add error message to chat
                chat_interface.add_message("assistant", f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

with tab3:
    st.header("ì§ˆë¬¸ ìƒì„±í•˜ê¸°")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'Raw Data ë™ê¸°í™”' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë™ê¸°í™”í•´ ì£¼ì„¸ìš”.")
    else:
        # LLM configuration for question generation
        st.subheader("LLM ì„¤ì •")
        
        col1, col2 = st.columns(2)
        
        with col1:
            question_model = st.selectbox(
                "Question Generation Model",
                ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4o", "gpt-4-turbo"],
                index=0
            )
        
        with col2:
            question_temperature = st.slider("ì§ˆë¬¸ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.9, step=0.1)
        
        # Number of questions
        num_questions = st.slider("ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜", min_value=1, max_value=10, value=5)
        
        # ì§ˆë¬¸ ìƒì„± ë§¤ë‹ˆì € ì„¤ì •
        question_manager = st.session_state.question_manager
        question_manager.set_llm(question_model, question_temperature)
        
        # Generate questions
        if st.button("ì§ˆë¬¸ ìƒì„±", type="primary"):
            try:
                with st.spinner("ì§ˆë¬¸ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    if st.session_state.db is None:
                        st.error("DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë™ê¸°í™”ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                    else:
                        # Generate questions using manager
                        questions = question_manager.generate_questions(st.session_state.db, num_questions)
                        
                        # Store generated questions
                        st.session_state.generated_questions = questions
                        st.session_state.edited_questions = questions.copy()
                        
                        st.success(f"{len(questions)}ê°œì˜ ì§ˆë¬¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
            except Exception as e:
                st.error(f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # Display and edit questions
        if st.session_state.generated_questions:
            st.subheader("ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜ì •")
            st.write("ì•„ë˜ì—ì„œ ì§ˆë¬¸ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            
            # Edit questions
            for i, question in enumerate(st.session_state.generated_questions):
                edited_question = st.text_area(
                    f"ì§ˆë¬¸ {i+1}",
                    value=st.session_state.edited_questions[i] if i < len(st.session_state.edited_questions) else question,
                    key=f"question_{i}"
                )
                
                # Update edited questions in session state
                if len(st.session_state.edited_questions) <= i:
                    st.session_state.edited_questions.append(edited_question)
                else:
                    st.session_state.edited_questions[i] = edited_question
            
            # Add/Remove questions
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ì§ˆë¬¸ ì¶”ê°€"):
                    question_manager.add_question("ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
                    st.session_state.generated_questions = question_manager.get_questions()
                    st.session_state.edited_questions = question_manager.get_questions()
                    st.rerun()
            
            with col2:
                if st.button("ë§ˆì§€ë§‰ ì§ˆë¬¸ ì‚­ì œ") and len(st.session_state.generated_questions) > 1:
                    question_manager.remove_question(len(question_manager.get_questions()) - 1)
                    st.session_state.generated_questions = question_manager.get_questions()
                    st.session_state.edited_questions = question_manager.get_questions()
                    st.rerun()
            
            # Display final questions
            st.subheader("ìµœì¢… ì§ˆë¬¸ ëª©ë¡")
            for i, question in enumerate(st.session_state.edited_questions):
                st.write(f"{i+1}. {question}")

with tab4:
    st.header("RAG í‰ê°€í•˜ê¸°")
    
    if st.session_state.db is None:
        st.warning("ë¨¼ì € 'Raw Data ë™ê¸°í™”' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë™ê¸°í™”í•´ ì£¼ì„¸ìš”.")
    elif not st.session_state.edited_questions:
        st.warning("ë¨¼ì € 'ì§ˆë¬¸ ìƒì„±í•˜ê¸°' íƒ­ì—ì„œ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.")
    else:
        # Retriever configuration
        st.subheader("ê²€ìƒ‰ê¸° ì„¤ì •")
        
        col1, col2 = st.columns(2)
        
        with col1:
            search_type = st.selectbox(
                "Search Type",
                ["mmr", "similarity", "similarity_score_threshold"],
                index=0
            )
        
        with col2:
            k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (k)", min_value=1, max_value=20, value=5)
        
        # Additional parameters based on search type
        if search_type == "mmr":
            col1, col2 = st.columns(2)
            with col1:
                lambda_mult = st.slider("ëŒë‹¤ ê°€ì¤‘ì¹˜", min_value=0.0, max_value=1.0, value=0.25, step=0.05)
            with col2:
                fetch_k = st.slider("Fetch K (ê²€ìƒ‰ í›„ë³´ ìˆ˜)", min_value=k, max_value=50, value=10)
            
            search_kwargs = {"k": k, "lambda_mult": lambda_mult, "fetch_k": fetch_k}
        
        elif search_type == "similarity_score_threshold":
            score_threshold = st.slider("ì ìˆ˜ ì„ê³„ê°’", min_value=-1.0, max_value=1.0, value=0.5, step=0.1)
            search_kwargs = {"k": k, "score_threshold": score_threshold}
        
        else:
            search_kwargs = {"k": k}
        
        # LLM configuration
        st.subheader("LLM ì„¤ì •")
        
        answer_model = st.selectbox(
            "Answer Generation Model",
            ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o", "gpt-4-turbo"],
            index=0
        )
        answer_temperature = st.slider("ë‹µë³€ ìƒì„± ì˜¨ë„", min_value=0.0, max_value=1.0, value=0.0, step=0.1)
        
        # RAGAS metrics selection
        st.subheader("RAGAS í‰ê°€ ì§€í‘œ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            use_faithfulness = st.checkbox("ì •í™•ì„±(Faithfulness)", value=True)
            use_answer_relevancy = st.checkbox("ë‹µë³€ ê´€ë ¨ì„±(Answer Relevancy)", value=True)
        
        with col2:
            use_context_precision = st.checkbox("ë¬¸ë§¥ ì •ë°€ë„(Context Precision)", value=True)
            use_context_recall = st.checkbox("ë¬¸ë§¥ ì¬í˜„ìœ¨(Context Recall)", value=True)
        
        # Display questions to be evaluated
        st.subheader("í‰ê°€í•  ì§ˆë¬¸ ëª©ë¡")
        for i, question in enumerate(st.session_state.edited_questions):
            st.write(f"{i+1}. {question}")
        
        # í‰ê°€ ë§¤ë‹ˆì € ì„¤ì •
        evaluation_manager = st.session_state.evaluation_manager
        rag_manager = st.session_state.rag_manager
        
        # RAG ë§¤ë‹ˆì € ì„¤ì •
        rag_manager.set_llm(answer_model, answer_temperature)
        rag_manager.set_retriever(st.session_state.db, search_type, search_kwargs)
        
        # Evaluate questions
        if st.button("RAG í‰ê°€ ì‹¤í–‰", type="primary"):
            try:
                with st.spinner("ì§ˆë¬¸ì„ í‰ê°€ ì¤‘ì…ë‹ˆë‹¤..."):
                    st.write("RAGASë¡œ í‰ê°€ ì¤‘...")
                    
                    # í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ì •
                    metrics_config = {
                        'use_faithfulness': use_faithfulness,
                        'use_answer_relevancy': use_answer_relevancy,
                        'use_context_precision': use_context_precision,
                        'use_context_recall': use_context_recall
                    }
                    
                    # í‰ê°€ ì‹¤í–‰
                    results_df = evaluation_manager.evaluate_rag_system(
                        st.session_state.edited_questions,
                        st.session_state.db,
                        rag_manager,
                        metrics_config
                    )
                    
                    # Store results
                    st.session_state.evaluation_results = results_df
                    
                    st.success("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
            except Exception as e:
                st.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # Display results
        if st.session_state.evaluation_results is not None:
            st.subheader("í‰ê°€ ê²°ê³¼")
            
            results_df = st.session_state.evaluation_results
            
            # Display dataframe
            st.dataframe(results_df)
            
            # Calculate and display average scores
            avg_scores = evaluation_manager.get_average_scores()
            
            if avg_scores:
                st.subheader("í‰ê·  ì ìˆ˜")
                col1, col2 = st.columns(2)
                
                avg_scores_items = list(avg_scores.items())
                with col1:
                    for i, (metric, score) in enumerate(avg_scores_items):
                        if i < len(avg_scores_items) // 2 + len(avg_scores_items) % 2:
                            st.metric(metric.replace("_", " ").title(), f"{score:.3f}")
                
                with col2:
                    for i, (metric, score) in enumerate(avg_scores_items):
                        if i >= len(avg_scores_items) // 2 + len(avg_scores_items) % 2:
                            st.metric(metric.replace("_", " ").title(), f"{score:.3f}")
                
                # Visualizations
                st.subheader("Visualizations")
                
                # Bar chart for each metric
                metric_columns = [col for col in results_df.columns if col not in ["question", "answer"]]
                for metric in metric_columns:
                    fig = VisualizationUtils.create_metric_bar_chart(results_df, metric)
                    st.pyplot(fig)
                
                # Radar chart for average scores
                radar_fig = VisualizationUtils.create_radar_chart(avg_scores)
                if radar_fig:
                    st.pyplot(radar_fig)
            
            # Q&A Display
            st.subheader("ì§ˆë¬¸ê³¼ ë‹µë³€")
            for i, row in results_df.iterrows():
                with st.expander(f"Q{i+1}: {row['question']}"):
                    st.write("**ë‹µë³€:**")
                    st.write(row['answer'])
                    
                    if avg_scores:
                        st.write("**ì ìˆ˜:**")
                        for metric in avg_scores.keys():
                            if metric in row:
                                st.write(f"- {metric.replace('_', ' ').title()}: {row[metric]:.3f}")

# Add sidebar information
st.sidebar.title("â„¹ï¸ ì •ë³´")
st.sidebar.markdown("""
### ì‚¬ìš© ë°©ë²•:
1. **Raw Data ë™ê¸°í™” íƒ­**: raw_data í´ë”ì™€ ìë™ ë™ê¸°í™”
2. **RAG í…ŒìŠ¤íŠ¸ íƒ­**: RAG ì‹œìŠ¤í…œê³¼ ì‹¤ì‹œê°„ ì±„íŒ…
3. **ì§ˆë¬¸ ìƒì„±í•˜ê¸° íƒ­**: ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ìë™ ìƒì„±
4. **RAG í‰ê°€í•˜ê¸° íƒ­**: RAGASë¡œ ì •ëŸ‰ì  í‰ê°€

### ë™ê¸°í™” ì›Œí¬í”Œë¡œìš°:
1. raw_data í´ë”ì— .txt/.pdf íŒŒì¼ ì¶”ê°€
2. 'ë™ê¸°í™” ìƒíƒœ í™•ì¸' ë²„íŠ¼ìœ¼ë¡œ ìƒˆ íŒŒì¼ í™•ì¸
3. 'ë™ê¸°í™” ì‹¤í–‰' ë²„íŠ¼ìœ¼ë¡œ ìë™ ì²˜ë¦¬
4. ì¤‘ë³µ íŒŒì¼ ë°©ì§€ ë° ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™”

### RAGAS í‰ê°€ ì§€í‘œ:
- **ì •í™•ì„±(Faithfulness)**: ë‹µë³€ì´ ì‚¬ì‹¤ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€
- **ë‹µë³€ ê´€ë ¨ì„±(Answer Relevancy)**: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€
- **ë¬¸ë§¥ ì •ë°€ë„(Context Precision)**: ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì–¼ë§ˆë‚˜ ì •ë°€í•œì§€
- **ë¬¸ë§¥ ì¬í˜„ìœ¨(Context Recall)**: ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì–¼ë§ˆë‚˜ ì™„ì „í•œì§€

### ìš”êµ¬ ì‚¬í•­:
- .env íŒŒì¼ì— OpenAI API í‚¤ í•„ìš”
- í•„ìˆ˜ Python íŒ¨í‚¤ì§€: streamlit, langchain, ragas ë“±
""")